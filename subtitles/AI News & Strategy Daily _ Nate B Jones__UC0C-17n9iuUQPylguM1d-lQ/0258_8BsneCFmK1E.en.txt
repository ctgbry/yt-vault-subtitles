[00:00]
Number one, prediction. Just predicting
the next word sounds really trivial, but
it's not. Fundamentally, if you have
scale and if you understand the
structure of language, you can encode a
vast amount of knowledge. You can build
up answers token by token that reflect
that structure, that reflect that scale.
And you can use model weights, which are
conditional probabilities, and they can
encode a tremendously dense information
set. They can encode long range
relationships. They can encode short
range relationships. They can talk about
grammatical similarities. They can talk
about cognates or meaning similarities.
They can encode even relationships we
don't fully understand. One of the most
interesting things about LLMs and
weights and encoding is that we have
learned more things about language than
we expected.