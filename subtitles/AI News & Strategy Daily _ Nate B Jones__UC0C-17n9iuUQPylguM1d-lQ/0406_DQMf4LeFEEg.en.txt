[00:00]
The third one, the third law, if you
can't explain why it failed, you haven't
owned the system. Again, we've
emphasized accountability with
engineering, but it is more important
now. AI systems are entering regulated
spaces, spaces where they have to be
able to explain what happened. Human
responsibility requires us to own the
explanation, the accountability, and
where the buck stops.
If you can't explain what happened in
your system to a very smart
non-engineer, then you probably don't
really understand your own system and
you probably didn't really engineer it.
And so these three laws are actually
designed to fit together. They're
designed to be three pieces of the
engineering life cycle, the new
engineering life cycle in the age of AI.
Number one is specification. what we
promise when we build a system, how we
write contracts that stick regardless of
probabilistic systems. That's the
invariant piece. Number two is
verification or measurement. How we
prove that we delivered something in
production. And number three is

[01:00]
production. And number three is
accountability or explanations. How you
take ownership of outcomes.