[00:00]
Now we actually have failures that are
harder to detect. AI can fail by
hallucinating. AI can fail by drifting.
It can still be functional but be
completely wrong. This is not a failure
mode we're used to. We need intelligent
failure detection. We need the ability
to monitor reasoning quality, not just
system health. And how you measure that
is going to depend on the kind of
inference you want to build into your
agentic system. But you got to measure
it. You've got to be able to detect
failures that occur that are not just
catastrophic program didn't launch
failures. And you have to build your
system not from the perspective of what
would happen if the whole thing went
down, but from the perspective of how
can the system work well if it's
difficult to detect a degradation in
reasoning quality. You need to assume
that you are moving from a fail fast
world to a subtle failure world where
the failure is going to be hard to

[01:00]
the failure is going to be hard to
detect. And so you need to think a lot
about how you monitor quality in that