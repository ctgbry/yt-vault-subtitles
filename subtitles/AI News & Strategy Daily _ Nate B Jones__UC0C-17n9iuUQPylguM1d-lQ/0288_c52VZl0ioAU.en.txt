[00:00]
How do we expect them to maintain
understanding across a lifetime of
experience, particularly when they're
not getting better at this? This is not
a new issue. I am not telling you about
something that did not exist when Chad
GPT launched and now it does. I'm
telling you about something that hasn't
gotten solved. This is a limitation of
our architectures that is partly a
function of physics. One of the things
that Google engineers have observed is
that it is incredibly computationally
intensive to use the full 1 million
token context window. I don't know if
you know this, but context scales
quadratically. In other words, as you
burn more tokens, if you if you send
more tokens through, it's a quadratic
equation that scales to the power of
four in order to process those tokens.