[00:00]
tesla ai day presented the most amazing
real world ai and engineering effort i
have ever seen in my life
i wrote this and it meant it
why was it amazing to me
no not primarily because of the tesla
bot it was amazing because i believe the

[00:16]
bot it was amazing because i believe the
autonomous driving task and the general
real world robotics perception or
planning task is a lot harder than
people generally think
and i also believed the scale of effort
in algorithm data annotation simulation

[00:30]
in algorithm data annotation simulation
inference compute and training compute
required to solve these problems is
something no one would be able to do in
the near term
yesterday was the first time i saw in
one place
just the kind and the scale of effort
that is a chance to solve this the

[00:47]
that is a chance to solve this the
autonomous driving problem and the
general real world robotics perception
and planning problem this includes the
neural network architecture and pipeline
the autopilot compute hardware in the
car
dojo compute hardware for training the

[01:01]
dojo compute hardware for training the
data and the annotation the simulation
for rare edge cases and yes the
generalized application of all of the
above beyond the car robot to the
humanoid form
let's go through the big innovations

[01:16]
let's go through the big innovations
the neural network each of these is a
difficult and i would say brilliant
design idea that is either a step or a
leap forward from the state of the art
in machine learning
first is to predict the vector space not

[01:30]
first is to predict the vector space not
in image space this alone is a big leap
beyond what is usually done in computer
vision that usually operates in the
image space in the two-dimensional image
the thing about reality is that it
happens out there in the
three-dimensional world and it doesn't

[01:45]
three-dimensional world and it doesn't
make sense to be doing all the machine
learning on the 2d projections of it
onto images
like many good ideas this is an obvious
one but a very difficult one
second is the fusion of camera sensor
data before the detections the
detections performed by the different

[02:01]
detections performed by the different
heads of the multitask neural network
for now the fusion is at the multi-scale
feature level
again in retrospect an obvious but a
very difficult engineering step of doing
the detection and the machine learning

[02:16]
the detection and the machine learning
on all of the sensors combined as
opposed to doing them individually and
combining only the decisions
third is using video contacts to model
not just vector space but time
at each frame concatenating positional

[02:31]
at each frame concatenating positional
encodings multi-cam features and ego
kinematics
using a pretty cool spatial recurrent
neural network architecture
that forms a 2d grid around the car
where each cell of the grid as a rnn
recurrent neural network the other cool

[02:45]
recurrent neural network the other cool
aspect of this is that you can then
build a map
in the space of rnn features
and then perhaps do planning in that
space which is a fascinating concept
andre carpathi i think also mentioned
some future improvements performing the

[03:01]
some future improvements performing the
fusion earlier and earlier in the neural
network so currently the fusion of space
and time are late in the network
moving the fusion earlier on
takes us uh further toward
full
end-to-end driving with multiple

[03:15]
end-to-end driving with multiple
modalities seamlessly
fusing integrating the multiple sources
of sensory data finally the place where
there's currently from my understanding
of the least amount of utilization of
neural networks is planning so

[03:30]
neural networks is planning so
obviously optimal planning in action
space is intractable so that you have to
come up with a bunch of heuristics you
can do those manually or you could do
those through learning so the idea that
was presented is to use neural networks
as heuristics in a similar way that

[03:45]
as heuristics in a similar way that
neural networks were used as heuristics
in the multicarlo tree search for mu 0
and alpha 0 to play different games to
play go to play chess this allows you to
significantly prune the search through
action space
for a plan that doesn't get stuck in the

[04:01]
for a plan that doesn't get stuck in the
local optima and gets pretty close to
the global optimum i really appreciated
that the presentation didn't dumb
anything down
but maybe in all the technical details
it was easy to miss just how much
brilliant innovation that was here
the move to predicting in vector space

[04:17]
the move to predicting in vector space
is truly brilliant of course you can
only do that if you have the data and
you have the annotation for it but just
to take that step
is already taking a step outside the box
of the way things are currently done in
computer vision
then

[04:30]
then
fusing seamlessly across
many camera sensors
incorporating time into the whole thing
in a way that's differentiable with
these spatial rnns
and then of course using that beautiful
mess of features
both on the individual

[04:45]
both on the individual
image side and the rnn side to make
plans using neural network architecture
as a heuristic
i mean all of that is just brilliant
the other critical part of making all of
this work is the data and the data
annotation first is the manual labeling

[05:01]
annotation first is the manual labeling
so to make the neural networks that
predict in vector space work you have to
label in vector space so you have to
create in-house tools and as it turns
out tesla hired in-house team of
annotators to use those tools to then
perform the labeling vector space and

[05:15]
perform the labeling vector space and
then project it out into the image space
first of all that saves a lot of work
and second of all that means you're
directly performing the annotation in
the space in which you're doing the
prediction obviously as was always the
case as is the case with self-supervised
learning auto labeling is the key to

[05:31]
learning auto labeling is the key to
this whole thing
one of the interesting thing that was
presented is the use of clips of data
that includes video imu gps odometry and
so on for multiple vehicles at the same
location and time
to generate labels of uh both the static

[05:45]
to generate labels of uh both the static
world and the moving objects and their
kinematics that's really cool you have
these little clips these buckets of data
from different vehicles and they're kind
of annotating each other you're
registering them together to then
combine

[06:00]
combine
a solid annotation of that particular
part of road at that particular time
that's amazing because the more the
fleet grows the stronger that kind of
auto labeling becomes
and the more edge cases you're able to
catch that way speaking of edge cases

[06:15]
catch that way speaking of edge cases
that's what tesla is using simulation
for is to simulate rare edge cases that
are not going to appear often in the
data even when that data set grows
incredibly large and also they're using
it for annotation of ultra complex
scenes where accurate labeling of real

[06:31]
scenes where accurate labeling of real
world data is basically impossible like
a scene with like a hundred pedestrians
which i think is the example they used
so i honestly think the innovations on
the neural network architecture and the
data annotation is really just a big
leap
then there's the continued innovation on

[06:46]
then there's the continued innovation on
the autopilot computer side the neural
network compiler that optimizes latency
and so on
there's uh
i think i remember really nice
testing and debugging tools
for like
variants of candidate trained neural

[07:01]
variants of candidate trained neural
networks to be deployed in the future
where you can compare different neural
networks together that's almost like
developer tools
for
to be deployed neural networks
and it was mentioned that uh almost 10
000 gpus are currently being used to

[07:15]
000 gpus are currently being used to
continually retrain the network i forget
what the number was but i think every
week or every two weeks the network is
fully retrained end to end
the other really big innovation but
unlike the neural network in the data
annotation this is in the future so to

[07:31]
annotation this is in the future so to
be deployed still it's still under
development is the dojo computer which
is used for training
so the autopilot computer is the
computer on the car that's doing the
inference and dojo computer is the thing
that you would have in a data center

[07:45]
that you would have in a data center
that performs the training of the neural
network there's a what they're calling a
single training tile that is nine
flops it's made up of d1 chips that are
built in house by tesla each chip with
super fast io each tile also with super

[08:01]
super fast io each tile also with super
fast io so you can basically connect an
arbitrary number of these together each
with the power supply and cooling
and then i think they connected uh like
a million nodes
to have a compute center i forget what

[08:15]
to have a compute center i forget what
the name is but it's 1.1 xflop
so combined with the fact that this can
arbitrarily scale
i think this is basically contending to
be the world's most powerful neural
network training computer again the
entire picture that was presented on ai

[08:31]
entire picture that was presented on ai
day is amazing
because the what would you call it the
tesla ai machine can improve arbitrarily
through the iterative data engine
process of auto labeling plus manual
labeling of edge cases so like that
labeling stage plus a data collection

[08:47]
labeling stage plus a data collection
retraining deploying and again you go
back to the data collection the labeling
retraining and deploying and you can go
through this loop as many times as you
want to arbitrarily improve the

[09:01]
want to arbitrarily improve the
performance of the network i still think
nobody knows how difficult the
autonomous driving problem is but i also
think this loop does not have a ceiling
i still think there's a big place for
driver sensing i still think you have to

[09:15]
driver sensing i still think you have to
solve the human robot interaction
problem to make the experience more
pleasant but damn it
this loop of manual and auto labeling
that leads to retraining at least the
deployment goes back to the data
collection and the auto labeling and the
manual labeling is incredible

[09:31]
manual labeling is incredible
second reason this whole effort is
amazing is that dojo can essentially
become an ai training as a service
directly taking on aws and google cloud
so there's no reason it needs to be
utilized specifically for the autopilot

[09:45]
utilized specifically for the autopilot
computer the simplicity of the way they
describe the deployment of pi torch
across these nodes you can basically use
it for any kind of machine learning
problem especially one that requires
scale finally the third reason all this
was amazing is that the neural network
architecture and data engine pipeline is

[10:01]
architecture and data engine pipeline is
applicable to much more than just roads
and driving it can be used in the home
in the factory and by robots basically
any form as long as has cameras and
actuators including yes the humanoid
form
as someone who loves robotics

[10:17]
as someone who loves robotics
the presentation of a humanoid tesla bot
was truly exciting
of course for me personally the lifelong
dream has been
to build the mind the robot that becomes
a friend and a companion to humans not

[10:30]
a friend and a companion to humans not
just a servant that performs
boring and dangerous tasks
but to me these two problems should and
i think will be solved in parallel
the tesla bot if successful just might
solve the latter problem of perception

[10:45]
solve the latter problem of perception
movement and object manipulation
and i hope to play a small part in
solving the former problem of human
robot interaction and yes friendship
i'm not going to mention love when
talking about robots

[11:01]
talking about robots
either way all of this to me paints a
picture of an exciting future
thanks for watching hope to see you next
time

[11:28]
you