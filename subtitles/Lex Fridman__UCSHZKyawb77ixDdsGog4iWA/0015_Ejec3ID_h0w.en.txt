[00:00]
so I'm going to take a picture so I
remember how many of you are
here
smile um like Sami says my name is Sher
Moore I work in the Google brain team um
so today I'll be giving a tutorial on
tensor flow first I'll talk a little bit
about what tensor flow is and uh how it
works how we use it at Google and then
the important part is that I'm going to
work with you together to build a couple
models to solve the most classic machine
learning problems so-called get your
feet vat for those of you from New
Zealand anybody from New
Zealand um so hopefully at the end
you'll be going home with all the tools
that you have to build all the wonderful
things that you have watched today like
all the image recognition the training
of different colors Arts making music

[01:00]
of different colors Arts making music
so that's the
goal so before I go any further has
everybody installed
tensorflow yay brilliant thank you and I
would like to acknowledge uh so I know
the link here says Sheran but if you
have wolf G uh TF tutorial is perfectly
fine wolf is actually the my colleague
who spent all the time verifying
installation on every single platform so
I would really like to thank him thanks
Wolf if you're you're are watching and
also I have my wonderful product boss or
product manager in the audience
somewhere so if you guys have any
request for tensorflow make sure that
you go find him and tell him why that
tensorflow must support this feature all
right Zach
somewhere all right so there he
is so um with that we can move forward
talk about
tensorflow so what exactly is tensorflow
tensorflow is a machine learning library

[02:00]
tensorflow is a machine learning library
that we developed uh at Google and we
open sourced it last
November and ever since then we have
become the most most popular machine
learning library on GitHub how do we
know because we have over
32,000
Stars those of you who track GitHub you
know how hard it is to get one of those
acknowledgement and we also have over
14,000 forks and we have over 800 no
8,000 contributions from 400 developers
400 individual
developers and we designed this
specifically for machine learning
however as you'll see later because of
its really flexible data flow
infrastructure um it makes it really
suitable for pretty much any application
that can fit into that model basically
if your model can be asynchronous and
fire on when data is ready you can

[03:00]
fire on when data is ready you can
probably use tensor
flow it originally we worked alongside
with all the researchers as a matter of
fact I was really fortunate when I
joined the team I sat right next to Alex
the person who invented Alex net so
that's how closely we work together as
we develop tensor flow they would tell
us no this is not how we use it yes when
you do this it makes our lives a lot
easier and this is why we believe that
we have developed uh an infrastructure
that will work really well for
researchers
and also being Google we also always
have in mind that we would like to take
from research to prototyping to
production in no time we don't want to
want you to write all the code that's
typically you know just throwing away we
want you to write code that can
literally cut and paste and saving the
file and and prze it immediately so
tensor flu is really designed with that
in
mind so we are halfway into your uh deep
learning school so can anybody tell me

[04:01]
learning school so can anybody tell me
if you want to build a
neuronet what must you have what are The
Primitives what are
the yeah primitive I think is the word
I'm looking for what must you have to
build a
neuronet anybody what is in the
neuronet answer very good answer so in
the neuronet you have neurons
that's that's right so
in and you need so all these neurons
what do they operate on what do all
these neurons
do they process data data and they
operate on data and they do something
such as convolution matrix
multiplication Max pulling average
pulling Dropout whatever that is so in
tensor flow all the data is held in
something called a tensor

[05:00]
something called a tensor
tensor is nothing more than a
multi-dimensional array for those of you
who are familiar with numpy arrays it's
very similar to the ND
array and the graph I think one of the
gentle minority this morning described
there's this concept of the graph which
is a composition of all these um neurons
that do different
functions and all these neurons are
connected to each other through their
inputs and outputs so as data become
aail available they would fire by fire
amend they do what they're designed to
do such as doing matrix multiplication
or convolution and then they will
produce output for the next uh
computation node that's comp connected
to the output so by doing this so I
don't know how many of you can actually
see
this uh animation yeah so this is to
really visualize how tensor flow works
all these notes the oval ones are
computation this rectangle ones are

[06:02]
computation this rectangle ones are
staple nodes so all these noes they
would generate output or they take input
and as soon as all the inputs for a
particular node are available it would
do its thing produce output and then the
tensor all the data which are how
intensiv will flow through your network
therefore tensor blow yeah
so everybody's like wow this sounds like
magic how does it work so who said is it
Sir Arthur Clark that says any
sufficiently what's the word any
sufficiently advanced technology is
indistinguishable from Magic so that's
what this is it's just really
awesome excuse me for a
second I know I want I want to get
through this as quickly as possible so
we can actually do the lab that you're

[07:00]
we can actually do the lab that you're
all dying to
do so as any good infrastructure so this
is I want to give you a little image of
you know how we Design This in uh tensor
flow just like any well-designed
infrastructure has to be really modular
because being modular allows you to uh
innovate to upgrade to improve to modify
to do whatever you want with any pce as
long as you keep the apis consistent
everybody can work in parallel it's
really empowering I think that's one of
the wonderful things that's done at
Google pretty much any infrastructure at
Google is really modular they talk
really about to each other all you need
to maintain this API um what stability
so in this case we have a friend end I
think you you guys must have seen some
examples of how you construct a graph so
we have the front end libraries written
your favorite language and if C++ and
python is not your favorite language
feel free to contribute we always
welcome contribution

[08:00]
welcome contribution
so you write you constru your graph in
your favorite language and this graph
will be sent to we call the core tens FL
execution system that's your run time
and that's what you all will be running
today on your laptop when you open your
uh your python notebook or Jupiter
notebook so the execution run time
depending on where you are going to run
this uh application it will send the
kernel to the corresponding device so it
could be a CPU could be a GPU could be
your phone could be TPU anybody knows
what TPU
is brilliant very nice I was say Strat
has say anybody knows what TPU is
everybody is like
translation so this is good so just to
highlight our portability today you'll
be running tensible on your laptop we
run it in our data center you everybody
they can run it on your iOS on your

[09:01]
they can run it on your iOS on your
iPhone your uh Android
phone I would love to see people putting
out on Raspberry Pi because can you
imagine you can just write your own
tensorflow
application it could be your security
system because you know somebody just
stole my bike and my security camera
capture all this grainy stuff that I
cannot tell wouldn't it be nice if you
do machine learning on this thing and it
just start taking high resolution
pictures you know when when things are
moving rather than constantly capturing
all the greeny images which is totally
useless so I think the application
literally applications are Limitless you
know your imagination is the
limit
so uh we talked about what tensor flow
is how it works how do we use it at
Google we use it everywhere I think you
have seen some of the examples we use it
to recognize pictures this is actually
done with Inception they can recognize
out of the box um one of a thousand

[10:02]
out of the box um one of a thousand
images you have to retrain it if you
wanted to recognize that your all your
relatives or you know your pads but it's
not difficult and we I have links you
know for you to actually if you want to
train on your own images it's really
easy that you should totally try it
would it be fun if you go to your
relative your 40y year reunion you just
go I know you who you are I know who you
are you know just show off a little it
would be brilliant and we also use it to
do go voice
search this is one that's super awesome
so how many of you use Smart reply have
you ever Ed smart reply yeah yeah this
is awesome especially for those of you
who doing what you're not supposed to do
you know texting while driving you you
saw an email coming in and you can just
say oh yes I'll be there you know so uh
based on the statistics that we
collected in February over 10% of the
all the responses sent on mobile is
actually done by our smart reply that's

[11:01]
actually done by our smart reply that's
a I I believe if we have um maybe Zach
can collect some stats for me later and
maybe by now it'll be like
80% it's actually really funny at the
very beginning when we train it the
first answer is always I love you we're
like that's probably the right not the
right
answer we also play games all of you I'm
sure have followed this there all kinds
of games that are being developed it's
really fun to watch if you watch it
literally come up with scenarios for you
to play as well it not only learns to
play the game but learns how to make a
game for you it's it's fascinating and
of course art and I think many of you
have done this deep dream if we have
time in the end of the lab we can we can
try this um so if we are super fast we
can all try the mix some
art and all those what I just talked
about of course Google being this
wonderful
generous company wants to share our

[12:02]
generous company wants to share our
knowledge so we have actually published
our our models so if you go to that link
you'll find all these
Inception and captioning um language
model on a billion words the latest rest
net on c410 sequence to sequence which I
think quac will be talking about
tomorrow and and there were we have many
other high level libraries so today my
lab the lab that we will do will be on
the cour tensor flow apis but there are
tons of new higher level apis such as uh
some of the mention cares and we have
slim we have pretty tensor we have TF
learn we have many libraries that's
developed on top of the core tensor flow
apis that we encourage people to do so
if whatever is out there does not fit
your needs perfectly go for it develop
your own and we welcome the contribution
we published a lot of that here I might
have blurred some of the boundaries but
these are basically all the models and

[13:00]
these are basically all the models and
libraries that we we have produced and
we really love
contribution um if you have developed a
really cool model please do send to us
and we would you know we would showcase
your uh your
work so that's the introduction of
tensorflow how does everybody feel are
you all ready to get
started all right so okay before you
bring up your python notebook I want to
say what we are going to to do first
okay so as I mentioned there are two
classic machine learning problems that
everybody does one is uh linear
regression the other is classification
so we're going to do two simple labs to
cover
those um I do have a lot of small
exercises you can play with it I
encourage you to play with it to be a
lot more comfortable so the first one is
linear
regression so I'm sure it has been
covered yeah in today's lectures
somebody must have covered linear
regression can you can anybody give me a

[14:01]
regression can you can anybody give me a
on line summary what is the linear
regression
problem
anybody the
professors well if you don't know go
Google
it so I didn't know the the audience um
you know when when Sami asked me to do
this so I wanted to I I wrote this for U
one of the high schools so I think it
still kind of makes sense right because
all of us have done have played this
game at one point of our lives like I
you know if you if you tell me five or
tell you 10 and you try to guess you
know what the equation is we must have
all done this I think my friends are
still doing on Facebook saying oh you
know only genius can solve this kind of
equation and then they would be like
yeah you know I solved it I was like my
God if anybody you know I will unfriend
you guys if you click on another one of
those so but basically this is what we

[15:00]
those so but basically this is what we
are trying to do in the first lab so we
will have a mystery equation it's really
simple it's just a linear you know
literally a line and then I will tell
you that this is you know the formula
but I'm not going to give you a weight w
and B you know all of you have learned
by now W stands for weight and and bias
b stands for bias so the idea is that if
you're given enough samples if you're
given enough X and Y values you should
be able to make a pretty good guess what
w and B is so that's what we are going
to do so now you can bring up your uh
Jupiter notebook if you don't have it up
already yeah everybody have it
up yes can I see a short of hands
everybody those of yeah brilliant all
right so um for pretty much any models
these are going to come up over and over

[16:00]
these are going to come up over and over
again and just to make sure that you're
all paying attention I do have
uh I asked him if I was supposed to
bring Shrek and he said no but I do have
a lot of tensorflow stickers and I have
all kinds of little toys so later I'm
going to ask this question whoever can
answer will get some mystery present so
really pay attention Okay so pretty much
with when whenever you build any model
there are I would say four things that
you will need you need input you need
data so you're going to see in both Labs
we're going to be defining some data
you're going to be build building an
inference graph I think in other
lectures is also called a forward graph
to the point that it produces
Logics the logistic outputs and then
you're going to have a training training
operations which is where you would
Define a loss an
Optimizer and
I think that's pretty much it hang on

[17:00]
I think that's pretty much it hang on
and there's the fourth
thing yeah and then you will basically
run the graph so the three important
things okay you'll always have your data
your inference graph you always have to
Define your loss and your
Optimizer and the training is basically
to minimize your loss so I'm going to be
asking that
later all right so now we know what
we're going to do so you can go start go
to that
lab yeah everybody have it so shift
return We Run the first one you say I
have no idea what's Happening Here
return again still nothing however let's
see what we are producing here so you
can also do the same on your laptop you
uncomment that plot you say so you know
what kind of data you're generating so
in this case when he return what are we
seeing this is your input data this is
when you try to make a guess when your
your friend tell me oh you know give me

[18:00]
your friend tell me oh you know give me
your X and Y so this is that you know
when your X is 0.2 you know your Y is
0.32 so this is basically your input
data yeah everybody
following if at any point you're kind of
lost raise your hand and your buddy next
to you will be able to help
you so now so oh okay I want to say one
more thing so today the laughs are all
on really core tensorflow apis the
reason I want to do that I know there
are a lot of people who would use carot
use uh another thing that we we heavily
advertised which is contri contri learn
so I feel like I'm giving you all the
ingredients so even though you could go
to Whole Foods and buy the package meal
you know maybe one day you don't like
the way they cook it so I'm giving you
all your lobsters your Kobe beef Okay so
that you can actually assemble whatever

[19:00]
that you can actually assemble whatever
you want to build
yourself so this um next one is very key
it's a very key concept here you see
variables so variable intensive flow is
how is corresponding to the square any
of you remember this slide okay I'm
going to switch quickly don't freak
out so actually I wanted you all to
commit this little graph to
to your memory because you'll be seeing
this over and over again and it makes a
lot more sense when you have this visual
uh
representation so intensive flow the way
we hold all the data the weights and the
biases associate associated with your
network is using something called
variable it's a state fold
operation I'm going to switch back
okay so this is what we are doing in
section 1.3 we are building
those uh Square nodes in your network to

[20:03]
those uh Square nodes in your network to
hold these weights and variables and
they are the ones when you train that's
where the gradients will be applied to
so that they will eventually
resemble the target Network that you're
are trying to to train
for so now you have built it wonderful
okay so you can shift return do you see
anything
no so exactly what have we built that's
un commmon a
look so these are called the variable
objects so at the bottom of the slide
for for this slab I have a link which is
our Google 3 docs the API docs which is
available in
GitHub I think you should always have
that up so whenever you want to do
something you would know what kind of
operations are possible with this object
for example I can say here what's the
name
of

[21:00]
of
this oh it's called variable six why is
it called variable six oh it's because
when I create this variable I didn't
give it a name so I can say
sheres
Sherry wait I hope that's
not but so see now my variable is called
Sherry
weight same thing with
my so this would be a good practice
because L later
bi oh because I ran this so many times
every single time you run if you don't
restart that it's going to continue to
grow your current path So to avoid that
confusion let me restart
it restart

[22:05]
I had to
sorry so now so we have done build our
input build our inference graph now we
can actually build our training graph
and as you have all learned we need to
define a loss function we need to Define
an Optimizer I think it's also called
something else reg regularizer maybe
some other terms and your ultimate goal
is to minimize your loss so I'm not
going to do it here but you can do it at
your at your leisure you can you know
uncommon all the these things that you
have created and see what they are and I
can tell you these are different
operations so that's how you actually
get to learn about the network that you
have built really well in the next line

[23:01]
have built really well in the next line
I'm also not going to uncommon but you
should at one point this is how you can
you can see what you have built so
actually why don't I do that because
this is really critical and as you debug
this will
become so this is the network that you
have built they have names different
names they have inputs and outputs they
have attributes and this is how we
connect all these nose together this is
your neuronet so what you have what
you're seeing right now is your neuronet
that you have just built yeah everybody
following so now the next step now
you're done right you build your network
you build all your training now you can
let's do some
training so in tensorflow do you
remember in the architecture that I
showed you have the front end C++ and
python front end you use that to build
your graphs and then you send the graph
to your runtime and this is exactly what

[24:00]
to your runtime and this is exactly what
we're doing here whenever you this is
how we talk to the runtime we create
something called a session you get a
handle to the session and then when you
say run you're basically sending this
session your graph so this is different
from the other uh machine learning
libraries I forgot which one those are
So-Cal imperative your happens as your
type tensorflow is different you have to
construct your graph and then you'll
create a session to talk to your runtime
so that it knows how to run different
devices that's a very important concept
because people constantly compare and
it's just
different okay so
now you can also accon that to see what
the initial values are but we're not
going to do that we're just going to run
it and now we're going to
train the data is not so what do you
think of the data
did we succeed in

[25:02]
guessing is everybody following what we
are trying to
do yeah yes
no so what was our object objective
before I started the lab what did I say
my our objective
was yes to guess the mystery function so
have we
suceeded it's really hard to tell all
right so now all of you can go go to the
end and un comment this part let's see
how successful we
are so the green line was what we have
initialized our weight and bias
to yeah the blue dots were the initial
value the target
values and the red dots is is our
trained value make sense so how

[26:02]
trained value make sense so how
successful are we great big success yeah
I would say so so any
questions any questions so far so what
are the things so everybody should play
with this you're not going to break it
this is a notebook python notebook the
worst you that happens is they just say
Okay clear all like what I just did and
change it so what can you play with
since today you learn all this Concepts
about different loss functions different
op optimizers all this crazy different
inputs different data so now you can
play with it how about instead of um
let's pick one um so instead
[Music]
of gradient descent what are the other
optimizers how can you find out I guess
that's a better question if I want to
know what other optimizers are available
in tensor Flor how can I find
out very good yes the GitHub good go
Google 3 the G3 dog link with the

[27:02]
Google 3 the G3 dog link with the
apis I'm going to switch one more tab
bear with me so this is when you go
there this is what you can find you can
find all the let me make it
bigger so you can find all the different
optimizers so you can play with that so
maybe uh gradient descent is not the
best Optimizer you can use so you go
there and say why are the other
optimizers
and then you can literally come here and
search
Optimizer or you can say wow you know I
have add the Delta ADR Adam I'm sure
there are more a momentum so we also
welcome contribution if you don't like
any of these please do you know go
contribute write a new Optimizer send a
pool request would love to have it so I
would like to say this over and over
again we love contribution it's an open
source project so keep that in mind we
would love to see your code or your
models on

[28:02]
models on
GitHub
so
um back to this
one how is everybody feeling this is too
simple yeah should we go W just
yes can I say that one
oh is that right heit tap to see all the
other optimizers you man oh brilliant
see I didn't even know that learn
something new every day let me go
there
tap
here oh
yay so this is even easier thank you
clearly I don't program in Notebook as

[29:02]
clearly I don't program in Notebook as
often as I should have so this is where
you can all the wonderful things that
you can do thank
you this is probably a little too low
level I think it has
everything but that's a very good tip
thank
you so anything else you would like to
see with linear regression is too simple
you guys all want to re recognize some
digits
all right so that sounds like a
consensus to me so let's move if you
just go to the bottom you can say click
on this
one so this is our M this
model so before we start the lab so once
again what are we trying to do
so we have all these handwritten digits

[30:00]
so we have all these handwritten digits
what does amness stand for does anybody
know what does amness stand
for very good see some of they can
Google very good so it has it's St I
think makes National Institute of
Standards and Technology something like
that so they have this giant collection
of digits so you know if you go to the
Post Office you already know that it's a
trivial it's a solved problem but I
don't know if they actually use machine
learning but our goal today is to build
a little network using tensor
flow that can recognize these
digits once again we will not have all
the answers right so all we know is that
the network the input will give us one
and then we'll say it's a
nine and then the uh the in and then we
have the so-call ground truth and then

[31:00]
have the so-call ground truth and then
they will look at it and say no you're
wrong and then we'll have to say okay
fine this is the difference we're going
to train the network that way so that's
our
goal yeah do everybody see the network
on the side so now we can go to the
lab so can anybody tell me why are the
three or four things that's really
important whenever you build a network
what's the first one
your data second
one inference graph third
one your train graph and with this lab
I'm going to teach you a little bit more
they are like the the rock you know like
when you go to restaurant I not only
give you your Lobster your your Kobe
beef I'm also going to give you a little
rock so you can cook it okay so in this
lab I've also teach some absolute
absolutely critical additional
infrastructure pieces such as how to
save a checkpoint how to load from a

[32:00]
save a checkpoint how to load from a
checkpoint and how do you evaluate your
network I think somebody at one point
asked how do you know the network is
enough you evaluate it to see if it's
good enough so those are the three new
pieces of information that I'll be
teaching you and also I'll teach you a
really really useful concept it's called
placeholder that was requested by all
the researchers we didn't used to have
it but they they all came to us and say
when I train I want to be able the feed
my network any Daya we want so that's a
really key concept that's really useful
for any practical training whenever you
start writing real training code I think
you that will come in handy so those are
the I think four Concepts now that I
will introduce in this lab that's
slightly U different from the previous
one how to save checkpoint how to load
from checkpoint how to run evaluation
and how to use placeholders I think the
placeholder is actually going to be the
first one so once again we have our
typical border play stuff so so that you
hi return you import a bunch of uh

[33:01]
hi return you import a bunch of uh
libraries the second one this is just
for um convenience I Define a set of
constants some of them you can play with
such as the maximum number of steps
where you're going to save all your data
how big the batch sizes are but some
other things that you cannot change
because of the data that I'm providing
you for example the mless
pictures any questions so far
so now we'll read some data is everybody
there in 2.3 I'm at 2.3 right now so now
I use if you don't have slash Tamp it
might be an issue but hopefully you
do if you if don't have SL slam change
the
directory um directory name so the next
one is where we build inference so can
anybody just glance and and tell me what
we're building
what kind of network how many layers am

[34:00]
what kind of network how many layers am
I
building I have two hidden layers you
have all learned hidden layers yeah
today and I also have a linear layer
which will
produce Logics that's correct so that's
what all the inference graphs will
always do they always construct your
graph and they produce logistic output
so once again here you can uncommon it
and see what kind of graph you have
built uh once you have done the whole
tutorial by
yourself you can actually run tensor
board and you can actually load this
scph that you have saved and you can
visualize it like what I have shown in
the slide I didn't draw that slide by
hand it's actually produced by tensor
board so you can see the connection of
all your notes so I feel that that
visual representation is really
important also it's very easy for you to

[35:01]
important also it's very easy for you to
validate that you have indeed build a
graph that you thought sometimes people
call something repeatedly and they have
generated this gigantic graph they were
like oh that wasn't what I meant so
being able to visualize is really
important any questions so far see here
I have good habits I actually gave all
my variables names once again the hidden
layer one hidden Layer Two they all have
weights and biases weights and biases
Etc so now we're going to build our
train graph so here
is actually here there's no New Concept
once again you define the loss function
we once again pick gradient descent as
our Optimizer we added a global uh sta
variable that's what we will use later
when we save our checkpoints so you
actually know at which point what
checkpoint this corresponds to otherwise
if you always save it to the same name
then later you know you said wow this
this this um result is so wonderful but

[36:01]
this this um result is so wonderful but
how long did it take you have no idea so
that's a a training concept that we
introduce it's called Global step
basically how long you have trained and
we usually save that with the checkpoint
so you know which checkpoint has the
best
information yeah everybody is good at
2.5 so now the next one is the
additional stuff that I just
mentioned oh that piece of Rock that I'm
giving you now to cook your stuff so one
is the placeholder so we are going to
Define two one to hold your image and
the other to hold your labels the we
build it this way so that we only need
to build a graph once and we will be
able to use it for both training
inference and evaluation later it's very
handy you don't have to do it this way
and one of the exercises I put in my
slide is to try to do it differently but
this is a very handy way and get you
very far with minimum work so as I said

[37:03]
very far with minimum work so as I said
in the slid I know I don't have any
highlighters beams but you see there
says after you created your placeholders
I said add to collection and remember
this op and later we'll see how we're
going to recall this op and how we're
going to use
it in the next one we're going to call
our inference build our inference
is everybody following this part okay
and once again we remember our
Logics and then we create our train op
and our loss up just like with linear
regression just like with the linear
regression we're going to initialize all
our variables and now at the bottom of
this cell that's the second New Concept
that I'm introducing which is the saver
this is what you will use to do
checkpoints to save the states of your

[38:01]
checkpoints to save the states of your
network so that later you can evaluate
it or if your training was interrupted
you can load from a previous checkpoint
and continue training from there rather
than always reinitialize all your
variables and start from scratch when
you're training really big networks such
as Inception is absolutely critical
because when I I think when I first
train Inception it took probably six
days and then later when we have
replicas it took still like stay of the
hour is still two and a half days you
don't want to have to start from scratch
every single
time so yeah everybody got that the
placeholder and the
saver so now it's 2.7 we're going to go
to
2.7 lot of code can anybody tell me
what's trying to do

[39:05]
so this is an yes so it's trying to
minimize
loss we can actually see
this so we'll run it once
okay where did I go
okay very fast it's done
but what if I really want to see what
it's
doing so python is
wonderful so I would like to actually
see did somebody show like how you know
your training is going well they show
the LW going down going down oh I think
my training is going really well so
we're going to do something similar
sorry so I'm going to create a variable
what do you call
losses which is just
uh an array so

[40:01]
uh an array so
here I'm actually going
to remember
it
pinned so what am I
collecting M plot
PL
lip anybody remember
this this a
plot let's try
this o look at
that now do you see your loss going down
so as you train your loss actually goes
down so this is how when you do large
scale training this is what we typically

[41:00]
scale training this is what we typically
do we have a gazillion of this javs
running in the morning we would just
glance at it and we know oh which one is
doing really really well so of course
you know that that's just when you're
are prototyping that's a really really
handy tool but I'm going to show you
something even
better oh that's part of the exercise
man I don't have it so as one of the
exercise I also put the answers in the
backup slides that you guys are welcome
to cut and paste into a cell then you
can actually run all all the evaluation
sets against your checkpoint so that you
know how well you're performing so you
don't have to rely on your eyes you know
glancing oh you my loss is going down or
relying on validating a single
image but see this is how easy it is
this is how easy the Prototype and you
can lar it very often our researchers
would cut and paste their Cod code and
put in a file and and that's basically

[42:01]
put in a file and and that's basically
their algorithm and they will publish
that with their paper they would uh send
it to our data scientists or production
people we would actually postize some of
their research this this is how easy
literally from research to prototyping
to production really streamlined and you
can do it in no
time so for those of you who have run
this step can you do an LS in your data
path wherever you you saved that where
wherever you declare your trainer to be
what do you see in
there
checkpoints that's right that's the
that's the money that's after all this
work all the all the rning all this
training on all this gazillion machines
that's where all your weights your
biases are stored so that later you can
you know load this network up and do
your uh Inception to recognize images to
Res reply to email to do art etc etc so

[43:05]
Res reply to email to do art etc etc so
that's really critical but how do we use
it have no fear all right let's move on
to 2.8 if you are not already there so
can somebody tell me what we're trying
to do
first that's right first we load the
checkpoint and you remember all the
things that we told the told our program
to
remember the Logics and the image
placeholder and the label placeholder
how are we going to use it now we're
going to feed it some images from our
evaluation and see what it thinks so now
if you hit
return what's the ground
truth five what's our
prediction three what's the actual image
could be three could be five you know

[44:02]
could be three could be five you know
but so the machine is getting pretty
close right I wouldn't I would say
that's a
three okay let's try a different
different one so you can hear return
again in the same cell oh I need to
somehow move this so what's the ground
truth this
time yeah I got it right so you can keep
hitting you know you can keep hitting
return and see you know how well is
doing but instead of validating you know
instead of hitting return 100 times and
count how many times I has gotten it
wrong as I said in one of the exercises
and I also put the answer in the slid
that you can cut and paste and actually
do a complete validation on the whole U
validation set but what do you think I
really so you can actually uh handw
write a different digit but the trick is
that a lot of people actually try that
and told me me it doesn't seem to work
so you remember in on the slide I said

[45:01]
so you remember in on the slide I said
this is what the machine sees this is
where I sees and this is what the
machine sees so in the amness data set
all the numbers are between zero and one
I
believe I could be wrong but I believe
it's between zero and one so if you just
use a random tool like your phone you
write the number and you upload it
number one the the picture might be too
big and need to scale it down um number
two it might have a different
representation sometimes is from 0 to
255 and you need to scale it you know to
the range that amnest that how you have
trained your network if you train your
network with those data and then you
should be able to recognize the same set
of data just like when we teach a baby
right if if you have never been exposed
to something you're are not going to be
able to recognize it just like with the
Oro one of our uh
colleagues um
caption uh what was that that program a
while ago anytime when it sees something

[46:00]
while ago anytime when it sees something
that it doesn't recognize have anybody
played with that captioning software
it's super fun so you can take a picture
and say you know two people eating pizza
or you know dog surfing but anytime it
sees something that it has never been
trained on it would say men talking on a
cell phone so for a while we had a lot
of fun with it we would put a watermelon
on on a post and they was say men
talking talking on the cell phone you
put a bunch of furniture in the room you
know with nothing and it was say men
talking on the cell phone so it was
really fun but just like with your
numbers if you have never trained it
with um that style like if I write
Chinese characters here it's never going
to recognize it but this is pretty fun
so you can play with it you know you can
see how well see every time see so far
it's 100% other than the first one which
I cannot tell
either so what are some of the exercises
that we can do here what do you want to
do with this lab it's too easy huh

[47:01]
do with this lab it's too easy huh
because I made it so easy because I
didn't know that you guys are all
experts by now otherwise I would have
done a much harder
La uh let me see what things we can
do so you can uncommon
[Music]
autographs oh so here's one actually you
already see it so try this can you guys
try saving the checkpoint say
car 100
steps and you're going to have a
gazillion but a tiny tiny checkpoints so
it's okay and try to run evaluation with
a different checkpoint and see what you
get do you know how to do that yeah
everybody know how to do
that so the idea is that when you run
evaluation is in a it's very very
similar so we typically run training in

[48:00]
similar so we typically run training in
evaluation in parallel or
validation so us it trains every so
often say every half an hour depending
on the the depending on your problem so
with the Inception every 10 minutes we
will also run evaluation to see how well
our model is doing so if our model gets
to say 78.6% which I believe is the
stateof thee art you'll be like oh my
mother's done training so that's how
that's why you want to save checkpoints
often and then you know validate them
often if you're done with that already
did you notice anything if you try to
load from a really early
checkpoint how is your uh how how how
good is it when it tries to identify the
the
digits just take a while
guess yeah very bad maybe every every
other one is wrong but this m this is
such a small data set it's very easy to
train and we have such a you know deep

[49:01]
train and we have such a you know deep
network if you only have one layer maybe
it won't get it
right um so another exercise I think all
these you can you can do after the lect
after this session is that really try to
learn to run EV um evaluation from
scratch rather than actually another
that part but run evaluation on the
complete validation set that's a that's
a really necessary skill to develop as
you build bigger models and you need to
run
validation so I think this is the end of
my my lab I do have bonus
Labs uh but I want to cover this first
the bottom line is that tensorflow is
really is for machine learning is really
from research to prototyping to
production it's really designed for that
and I really hope everybody in the
audience can I give it a try and if
there are any features that you find it

[50:00]
there are any features that you find it
lacking that you would like to see in
implemented either send us pool requests
we always welcome contribution or talk
to my wonderful product manager Zach
sitting over there he's taking requests
for
features so with that um yeah thanks and
have fun
[Applause]
thank you Sher we have time for
questions for those who actually tried
it see a so well done everybody feel
like they're experts they're all ready
to go make arts now right right go deep
dream you
know
cool if there are no questions uh I oh
there's one question I think someone
who's trying
desperately hi uh my name is pichin and
first of all thank you for the for

[51:00]
first of all thank you for the for
introducing tensor flow and for
Designing it I have two questions so the
first questions is I know the tensor
Flor have C++ API right so let's say if
I use kiras or any of the Python front
end I train a model can I does tensor
flow support that just I can pull out
the
C++ model of it and then just use that
yeah yes you can so even if I use for
example Kira's custom layer that I code
using python I still can get those
things that's correct oh is co it's just
the front end that's different how you
construct the graph ah nice but I we we
are not as complete on our C+ plus API
design for example a lot of the training
libraries are are not complete yet so
but for the simple models yes you can
you can so well let's say not the
training but let's say if I just want
the testing part because I don't need to
do I mean the training I can always do
in Python yes we we do have that already

[52:01]
in Python yes we we do have that already
actually if you go to our website
there's a label images. CC I think
that's literally just loading from
checkpoint and run the inference in C
that's all written in C++ so that's a
good example to follow cool uh a second
one so another thing that I noticed that
you support almost everything except
Windows everything except what I mean
iOS Android have no fear actually we
we're actively doing that but when I
first joined the team I think there were
10 of us and we have to do everything
like before open sourcing all of us were
in the in the in the conference room
together we're all riding dog we're
fixing everything so now we have more
people um that's like top of our list we
would love to support it so so so I'm
just curious because I I mean when I
look at the road map I didn't see a
clear timeline for Windows but uh thing
I know that just like the reason why you
cannot support Windows is because of
basil basil doesn't support Windows so

[53:03]
basil basil doesn't support Windows so
let's say theoretically or I mean what
you think just like I know basil that
just like it will get window five at
some somewhere November that is what
they say so once B can run in Windows
can I expect like just I can immediately
do tensor Flor or do you fores some
other problem maybe Zach would like to
take that
question okay
okay that's so yeah let's talk offline
yeah sure thank you very much sorry hi
uh great presentation and session uh my
name is Yuri I have a question about
tpus uh are they available right now for
testing and playing uh for non-google
employes
uh are we
TPU available I don't think so at the
moment and do do you know when it might
be available in the Google CL would you
like to take that

[54:03]
one I'm so glad we have a product boss
here so that he can sorry okay thank
you nice tutor I have a question are
there any like plans to integrate uh
tensorflow uh with the like open source
framework like myos and hdfs to make the
distribute tensor flow
EAS so there are definitely plans we are
Al always actively working on new
features but we cannot provide a solid
timeline right now so that we do have
like like this you know we do have plans
we do have projects in progress but we
cannot commit on the
timeline so I cannot give you a time
saying yes by November you have what you
know so thank you but uh if you have
this type of question I think Zach is
the best person to answer

[55:02]
to oh
hi I was wondering um does sensor flow
have any examples to load your own data
of what which data um so the current
example has a mest um data set are there
examples out there to load your own data
set yes yes definitely uh I think we
have two one's called The tensorflow
Poet I think that one that example shows
you how you can L your own data
set
um I
think is there another one uh Zach are
you aware of another one that might be
loading your own data set I know we have
retraining model you know if you go to
tensor flow we have an example to do
retraining those you can download from
anywhere so in our example we just
downloaded a bunch of
flowers so you can definitely download
whatever pictures that you want to
retrain and
go hello thank you for your presentation

[56:02]
go hello thank you for your presentation
uh I have a question concerning the the
training you can't you can train using
thr tensor for in any uh virtually in
any system like Android uh and uh what
about the model is do you provide
anything to move the model to Android is
the because um we generally you program
in Java there and yes so that's a
beautiful thing you remember the ecture
that I showed you build a model and then
just send it to the runtime it's the
same model running on any of the
different platforms it it can be a
laptop Android do you have your your own
specific format for the model or it's
just you you build the same because the
model is just bunch of Matrix Matrix and
values is there any special special
format for your model because I
sometimes it will it is bigger
so I would not recommend training C
Inception on your phone because all the

[57:00]
Inception on your phone because all the
convolution in the back problem will
probably kill it 10 times over so
definitely so there will be that type of
limitation like U I think you guys
talked about the number of parameters if
it blows the memory footprint on your
phone it's just not going to work and if
the compu like especially for
convolution it uses a lot of computers
yes that's for training but but for
inference you can run it anywhere okay
thank yeah it's the same model you just
restore actually are examples like um
label image that's the C++ version I
think I also wrote one it's called
classify image it's in Python that's
also you can run it on your phone so any
of these you can write your own and load
the checkpoint and run it on your phone
as well so definitely I encourage you to
do that thank you
cool yeah hi I have a question related
to tensorflow serving so uh I went
through the the online documentation and
um currently I think it requires some

[58:00]
um currently I think it requires some
coding in C++ and then combined with
python uh is there only going to be you
know only python Solution that's going
to be provided or is it always going to
be you know I think you need to do some
First Step you know to create a module
and then you know just import it into
python I am actually surprised to hear
that because I'm pretty sure that you
can write the model in just python or
just C++ you don't have have to write it
in one way or the other they might have
a special exporter tool at one point
that was the case they wrote their
exporter in C++ I think that's what
probably what you were talking about but
you don't have to build it in any uh
specific way the model is just you can
write in whatever language you like as
long as it produces that graph uhuh and
and that's all it
needs so so tensor flow surveying the
tutorial actually if you go on the side
it had those steps actually okay so I
will look into that so maybe you can

[59:01]
will look into that so maybe you can
come find me later and I I'll see what
the situation I do know that at one
point they were writing the exporter in
C++ only but that should have changed by
now because we are we're doing another
version of uh serving tensor serving and
is there any plan to you know provide uh
um apis for other uh languages like you
know like mxnet has something called
mxnet JS and you you know um you mean at
the front end front end yeah yeah we
have go I think we have go we have some
other languages maybe Zach can speak
more to it and once again if those
languages are not our favorite please do
contribute and if you would like us to
do it talk to Zach and maybe he can put
that you know maybe I don't know because
as somebody asked for the Android you
need Java FR so I think that's going to
help out in integrating these models
with that's yeah that's great great
feedback will definitely take note thank
you thank you hi I have a question uh

[1:00:01]
you thank you hi I have a question uh
I'm having a embedded GPU board tx1
which is a arm processor and I really
wanted to work with the tensor flow but
I got to know that it can only run on
x86 boards okay so when can we expect
the tensor flow can support arm
processors uh we will have to get back
to you after I have consulted with my uh
prodct boss say when we can add that
support thank you sorry one last
question um thanks for the presentation
Sher I have a question regarding the um
when you have the uh model and you want
to run inference is it is it possible to
make an executable out of it so you can
drop it into a a container or run it
separately from serving is that is that
something that you guys are looking into
just run the inference yeah just have it
as an as a binary yeah yeah you can
definitely do that right now you can

[1:01:00]
definitely do that right now you can
yeah you can all you you you are always
able to do that you mean just save you
meant just save
the you want to what I mean is that if
you can um package it into a single uh
binary source that you can just pass
around yes yes we actually do that today
that's how the label image works it's
just its own individual binary okay
actually converted all the checkpoints
into constants so it doesn't even need
to do the slow Etc it just RS a bunch of
constants and runs it so it's super
fast you cool okay let's uh thank Sher
[Applause]
[Music]
again we're going to take a short break
of 10 minutes let me remind you for
those who haven't noticed yet but all
the slides of all the talks will be
available on the website so do not worry
they will be available at some point as
soon as we get them from the speakers oh
I forgot to ask my bonus question but in
any case I have a lot of tensor flow
stickers up here if you would like one

[1:02:00]
stickers up here if you would like one
to put proudly display on your laptop
come get it