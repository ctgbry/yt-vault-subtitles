[00:01]
Thank you everyone
for braving the cold, and the snow
To be here
This is 6.S094: Deep Learning for Self-Driving Cars
And, it's a course
where we cover the topic of Deep learning

[00:19]
Which is a set of techniques,
that have taken a leap in the last decade
For our understanding
Of what artificial intelligence systems
are capable of doing

[00:30]
And self-driving cars, which is systems,
that can take these techniques,
and integrate them
In a meaningful, profound way
into our daily lives
In a way that transforms society.

[00:46]
So that's why both of these topics,
are extremely important
And extremely exciting.
My name is Lex Fridman,
And I'm joined by
an amazing team of engineers,
In Jack Terwilliger, Julia Kindelsberger, Dan Brown

[01:02]
Michael Glazer, Li Ding,
Spencer Dodd and Benedikt Jenik,
Among many others...
We build autonomous vehicles,
here at MIT,
Not just ones that perceive,
and move about the environment,

[01:20]
But ones that interact, communicate,
and earn the trust,
And understanding of human beings
inside the car,
The drivers and the passengers,
And the human beings outside the car

[01:32]
the pedestrians and other drivers
and cyclists.
The website for this course:
selfdrivingcars.mit.edu
if you have questions, email at:
deepcars@mit.edu

[01:47]
Slack: deep-mit
For registered MIT students,
you have to register on the website
And, by midnight,
Friday, January 19th

[02:02]
build a neural network,
and submit it to the competition.
That achieves the speed of 65 miles per hour
On the new deep traffic 2.0
It's much harder and much more interesting
than last year's
for those of you who participated.

[02:17]
There's three competitions in this class:
Deep Taffic,SegFuse
DeepCrash
There's guest speakers,
that come from:
Waymo, Google, Tesla

[02:31]
And, those are starting new,
autonomous vehicle startups
In Voyage, NuTonomy and Aurora
And then use a lot today from CES.

[02:48]
And, we have shirts!
For those of you who braved the snow
and continued to do so
towards the end of the class
there will be free shirts.
Yes, I said free and shirts
in the same sentence,

[03:00]
You should be here.
Okay. First: The Deep Traffic competition
There's a lot of updates, and we'll
cover those on Wednesday.
it's a deep reinforcement learning
competition.
Last year we received
over 18,000 submissions,

[03:18]
This year we're going to go bigger!
Not only can you control one car,
with your neural network
You can control up to ten
This is multi agent deep renforcement learning.

[03:30]
This is super cool!
Second: SegFuse - Dynamic
Driving Scene Segmentation competition
Where, you're given the raw video,
The kinematics of the vehicles,
the movement of the vehicle,

[03:49]
The state-of-the-art segmentation.
For the training set you're given:
Ground truth labels, pixel level labels
Scene segmentation,
and optical flow.

[04:00]
And with those pieces of data,
You're tasked to try to perform better
than the state-of-the-art
In image based segmentation.
Why is this critical,
And fascinating,
in an open research problem?

[04:17]
Because, robots that act in this world,
In the physical space,
not only must interpret,
Use these deep learning methods
to interpret,
The spatial visual characteristics of a scene,
They must also interpret, understand,
and track

[04:32]
The temporal dynamics of the scene.
This competition is about
temporal propagaton of information,
Not just scene segmentation.
You must understand the space,
and time.
And finally

[04:47]
Deep Crash
Where we use deep reinforcement learning,
To slam cars thousands of times,
Here, at MIT, at the gym.
You're given data on a thousand runs,
where car

[05:02]
Or a car knowing nothing
is using a monocular camera's
Single input,
driving over 30 miles an hour,
Through a scene,
it has very little control through
Very little capability
to localize itself
It must act very quickly.

[05:15]
In that scene you're given
a thousand runs, to learn anything.
We'll discuss this,
in the coming weeks.
This competition will result in
four submissions
That; We evaluate everyone's
in simulation

[05:32]
But the top four submissions,
we put head-to-head at the gym.
And, until there is a winner declared,
we keep slamming cars
...at 30 miles an hour.
Deep crash, and also on the website
is from the last year,

[05:47]
And on GitHub there's DeepTesla.
Which is using the large-scale
naturalistic driving data set
We have to train a neural network
to do enter and steering
That takes in monocular video
from the forward roadway,

[06:00]
And produces steering commands,
Steering commands for the car.
Lectures: Today we'll talk about
deep learning,
Tomorrow we'll talk about
autonomous vehicles,
Deep RL is on Wednesday,

[06:15]
Driving scene understanding
So segmentation
That's Thursday.
On Friday, we have Sacha Arnoud,
The Director of Engineering at Waymo.
Waymo is one of the companies,
that's truly taking

[06:30]
Huge strides in
fully autonomous vehicles.
They're taking the fully L4, L5,
autonomous vehicle approach.
and it's fascinating to learn,
he's also the head of perception
for them.
To learn from him; What kind of problems
they're facing?

[06:45]
And what kind of approach
they're taking on?
We have Emilio Frazzoli,
Who's one of last year's
speakers
Sertac Karaman said Emilio is
the smartest person he knows,
So Emilio Frazzoli's the CTO
of nuTonomy
An autonomous vehicle company, that was just acquired by Delphi

[07:03]
For a large sum of money.
And they're doing a lot of
incredible work in
Singapore, and here in Boston.
Next Wednesday,
we are going to talk
about the topic of our research,
or my personal fascination,

[07:17]
is deep learning for
driver state sensing.
Understanding the human,
perceiving everything about the human
being inside the car,
and outside the car.
One talk, I'm really excited about,
is Oliver Cameron on Thursday.

[07:32]
He is now the CEO of
autonomous vehicle startup Voyage.
He's previously the director of
the self-driving car program, for Udacity
He will talk about: how to start
a self-driving car company,

[07:46]
For those, who said that MIT folks
are entrepreneurs.
If you want to start one yourself,
he'll tell you exactly how.
It's super cool!
And then, Sterling Anderson!
Who was the director previously,
Tesla Autopilot team.

[08:03]
And now is a co-founder of Aurora,
The self-driving car startup,
that I mentioned,
...that has now partnered,
with NVIDIA and many others.
So, why self-driving cars?

[08:16]
This class is about applying
data-driven learning methods,
To the problem of autonomous vehicles.
Why self-driving cars are fascinating,
And an interesting problem space?

[08:30]
Quite possibly, in my opinion,
This is the first wide reaching,
and profound integration
Of personal robots, in society.
Wide-reaching, because there's
one billion cars on the road,
Even a fraction of that, will change,

[08:47]
...the face of transportation,
and how we move about this world.
Profound, and this is an
important point,
...that's not always understood.
There's an intimate connection,
between a human,

[09:02]
And a vehicle, when there's
a direct transfer of control.
It's a direct transfer of control...
That takes that, his or her life,
into the hands,
Of an artificial intelligence system.

[09:16]
I showed a few quick,
Quick clips here, you can Google
first time with Tesla autopilot,
On YouTube and watch people,
perform that transfer of control,
There's something magical...
About a human and a robot
working together,

[09:33]
That will transform,
what artificial intelligence is,
In the 21st century.
And this particular autonomous system,
AI system, self-driving cars,
is on the scale.

[09:46]
And the profound,the life-critical
nature of it, is profound.
In a way that, it will truly test
the capabilities of AI.
There'a a personal connection,
That will argue throughout these lectures,

[10:00]
That we cannot escape considering the human being.
That will argue throughout these lectures,
That we cannot escape considering the human being.
That autonomous vehicle, must not only
perceive and control
It's movement
through the environment.
You must also perceive everything
about the human driver and the passenger
And interact, communicate,
and build trust with that driver.

[10:20]
Because,...
In my view,
As I will argue throughout
this course,
An autonomous vehicle is more
of a personal robot,

[10:31]
than it is a perfect perception
controled system.
Because, perfect perception
and control,
For this world, full of humans,...
Is extremely difficult.
And could be, two-three-four decades away.

[10:47]
Full autonomy.
Autonomous vehicles are going to be flawed.
They're going to have flaws...
And we have to design systems,
that are effectively caught
That effectively transfer control
to human beings,

[11:02]
When they can't handle the situation.
And that transfer of control...
Is an...
Is a fascinating opportunity for AI.
Because the obstacle avoidance,

[11:16]
Perception of obstacles,
and obstacle avoidance,
It's the easy problem.
It's the safe problem.
Going 30 miles an hour
Navigating through streets of Boston,
It's easy.

[11:30]
It's when you have to get,
to work, and you're late.
Or you're sick of the person
in front of you,
...that you want to go
in the opposing lane,
and speed up.
That's human nature.
And we can't escape it.
Our artificial intelligence systems,

[11:47]
Can't escape human nature,
they must work with it.
What's shown here,
is one of the algorithms,
We'll talk about next week,
for cognitive load.
Or we take, the raw,...
3D convolutional neural networks,

[12:00]
Take in the eye region, the blinking,
and the pupil movement
To determine the cognitive load
of the driver.
We'll see how we can detect
everything about the driver,
Where they're looking? Emotion?
Cognitive load? Body pose estimation?

[12:16]
Drowsiness.
The movement towards full autonomy
...is so difficult...
I would argue
That it almost requires
human level intelligence.

[12:30]
That the....
As I said, 2-3-4 decade out
journey...
For artificial intelligence researchers,
to achieve full autonomy
Will require achieving, solving,
some of the problems
Fundamental problems
of creating intelligence.

[12:46]
And... That's something
we'll discuss,
In much more depth,
In a broader view in two weeks,
For the artificial general intelligence
course,
Where we have Andrej Karpathy,
from Tesla,
Ray Kurzweil, Marc Raibert,
from Boston Dynamics

[13:03]
Who asked for the dimensions of this room,
because he's bringing robots
Nothing else was told to me...
It'll be a surprise.

[13:16]
So that is why I argue
the human centered
Artificial intelligence approach
In every algorithm of a design
considers the human.
For autonomous vehicle on the left,
the perception
Scene understanding,
and the control problem,

[13:32]
As we'll explore through
the competitions,
And the assignments, of this course
Can handle 90, and increasing
...percent of the cases.
But it's the 10,
1.1 percent of the cases
as we get better and better,

[13:48]
That we have to...
We're not able to handle
through these methods
And that's where the human,
perceiving the human
is really important.
This is the video from last year.
Of Arc de Triomphe
Thank you

[14:00]
Didn't know it last year, I know now.
That is one of millions of cases,
Where human to human interaction
is the dominant driver.
Not, the basic perception
control problem

[14:19]
So why deep learning in this space?
Because deep learning
Is a set of methods,
that do well from a lot of data.

[14:31]
And to solve these problems
Where human life is at stake,
We have to be able to have techniques
That learn from data,
learn from real-world data.
This is the fundamental reality
of artificial intelligent systems

[14:45]
That operate in the real world.
They must learn from real world data.
Whether that's on the left
for the perception, the control side,
Or on the right, for the human
The perception, and the communication,
Interaction

[15:00]
And collaboration with the human,
And the human robot interaction.
Ok.
So what is deep learning?
It's a set of techniques,

[15:15]
if you allow me the definition,
of intelligence
Being the ability to accomplish
complex goals,
Then I would argue,
definition of understanding
Maybe a reasoning is...
The ability to turn complex information

[15:30]
Into simple, useful,
actionable information.
And that is what deep learning does.
Deep learning is representation learning,
Or feature learning, if you will.
It's able to take raw information,

[15:46]
Raw complicated information,
That's hard to do anything with,
And construct hierarchical representations
of that information,
To be able to do
something interesting with it.
It is the branch of artificial intelligence,
Which is most capable and focused,
on this task.

[16:04]
Forming representations from data,
Whether it's supervised
or unsupervised,
Whether it's with the help of humans,
or not;
It's able to construct structure,
Find structure in the data;
Such that you can extract

[16:18]
Simple, useful, actionable information.
On the left,
From Ian Goodfellow's book,
Is the basic example of
a misclassification.

[16:30]
The input of the image,
On the bottom, with the raw pixels
And as we go up the stack
as we go up the layers,
Higher and higher order representations
are formed.
From edges, to contours
The corners, to object parts

[16:46]
And then finally,
The full object semantic classification,
of what's in the image
This is representation learning
A favorite example for me
Is, one from four centuries ago.

[17:02]
Our place in the universe,
And representing that place
in the universe,
Whether it's relative to Earth,
Or relative to the Sun.
On the left is our current belief,

[17:16]
On the right is the one,
that was held widely,
Four centuries ago
Representation matters!
Because,what's on the right
Is much more complicated
than what's on the left.

[17:34]
You can think of,
in a simple case here
When the task is to draw
a line that separates,
Green triangles and blue circles
In the Cartesian coordinates space,
on the left
The task is much more difficult.
Impossible, to do well

[17:47]
On the right, it's trivial,
in polar coordinates.
This transformation is exactly
Whan we need to learn,
this is representation learning.
So you can take the same task,
Of having to draw a line
that separates

[18:00]
The blue curve, and the
red curve on the left .
If we draw a straight line,
it's going to be a high
There's no way to do it
with zero error.
With 100% accuracy
Shown on the right, is our best attempt.

[18:18]
But what we can do with deep learning,
With a single hidden layer network
done here,
Is form the topology,
the mapping of the space,
In such a way, in the middle,

[18:30]
That allows for a straight line to be drawn,
That separates the blue curve,
and the red curve.
The learning of the function
in the middle,
Is what we're able to achieve
with deep learning.
It's taking raw, complicated information,

[18:45]
And making it simple,
actionable, useful.
And the point is, that,
this kind of ability to learn,
From raw sensory information
Means that, we can do a lot more,
with a lot more data.

[19:03]
So, deep learning
gets better with more data.
And that's important,
for real world applications.
Where edge cases are everything.

[19:17]
This is us driving, with two
perception control systems.
One is in Tesla vehicle,
with the autopilot
Version one system that's
using a monocular camera,
To perceive the external environment,
And produce control decisions.

[19:31]
And our own, neural network
Running on adjacent TX2,
that's taking in the same.
With a monocular camera,
and producing control decisions.
And, the two systems argue,
and when they disagree
They raise up a flag, to say that
this is an edge case

[19:47]
That needs human intervention.
There is...
Covering such edge cases,
using machine learning,
Is the main problem,
of artificial intelligence, and...
When applied to the real world,

[20:01]
It is the main problem to solve.
Okay.
So what are neural networks?
Inspired very loosely, and I'll discuss
About the key difference
between,
Our own brains and artificial brains

[20:17]
Because there's a lot of insights,
in that difference.
But inspired loosely by
biological neural networks,
Here, as a simulation of a...
Thalamocortical brain network,
Which is only 3 million neurons,

[20:31]
476 million synapses...
The full human brain,
Is a lot more than that.
A 100 billion neurons,
...1,000 trillion synapses.

[20:46]
There's inspirational music,
with this one
That I didn't realize was here,
it should make you think.
Artificial neural networks,
yeah... Let's
Just let it play...

[21:00]
The human neural network is,
a hundred billion neurons, right?
1,000 trillion synapses.
One of the state-of-the-art,
Neural network is ResNet-152,
which has...
60 million synapses.

[21:17]
That's a difference, of about...
A seven order of magnitude difference
That's a difference, of about...
A seven order of magnitude difference
The human brains have,
10 million times more synapses,
Than artificial neural networks.
Plus or minus one order of magnitude,
depending on the network.

[21:34]
So, what's the difference, between
A biological neuron, and
an artificial neuron?
The topology of the human brain
have no layers.
Neural networks are stacked in layers

[21:46]
They're fixed, for the most part.
There is chaos!
Very little structure in our human brain
In terms of how
neurons are connected.
They're connected, often,
to 10,000 plus other neurons.

[22:00]
The number of synapses,
from individual neurons
That are...
Input into the neuron is huge!
They're asynchronous.
The human brain works asynchronously.
Artificial neural networks work synchronously.

[22:15]
The learning algorithm
for artificial neuron networks,
The only one, the best one...
Is back propagation.
And we don't know, how human brains learn...

[22:33]
Processing speed,
this is one of the...
The only benefits we have
with artificial neural networks is...
Artificial neurons are faster.

[22:45]
But they're also extremely power inefficient,
And... There is a division into stages,
Of training and testing with neural networks.
With biological neural networks,
as you're sitting here today

[23:00]
They're always learning.
The only profound similarity,
the inspiring one
The captivating one, is that both are,
Distributed computation at scale.
There is an emergent aspect
to neural networks,

[23:19]
Where the basic element of computation:
A neuron,
Is simple.
Is extremely simple.
But when connected together,
beautiful

[23:30]
Amazing, powerful approximators
can be formed.
A neural network is built up
with these computational units,
They're the inputs,
There's a set of edges,
with weights on them.
The edges...
The weights are multiplied
by this input signal,

[23:47]
A bias is added,
with a nonlinear function.
That determines whether the network
gets activated or not
Well, the neuron gets
activated or not.
Visualized here.
And these neurons can be combined
in a number of ways.

[24:03]
they can form a feed-forward neural network,
Or they can feed back into itself,
To form... To have state memory.
In Recurrent neural networks.

[24:15]
The ones on the left, are the ones
that are most successful,
For most applications, in computer vision.
The ones on the right are very popular,
and specific.
One temporal dynamics,
or dynamics time series

[24:30]
of any kind are used.
In fact, the ones on the right,
are much closer
To the way our human brains are
Than the ones on the left,
But that's why, they're really hard to train.

[24:45]
One beautiful aspect,
of this emergent power,
For multiple neurons
being connected together
Is the universal property
That with a single hidden layer
These networks can learn any function
Learn to approximate any function.

[25:00]
Which is an important property
to be aware of, because
The limits here, are not in the
power of the networks
The limit in...
...is in the methods by which
We construct them, and train them.

[25:21]
What kinds of machine learning,
deep learning are there?
We can separate into two categories.

[25:32]
Memorizers,
The approaches, that essentially
memorize patterns in the data.
And approaches that,
we can loosely say
Are beginning to reason

[25:45]
To generalize over the data,
with minimal human input.
On top, on the left are the,
quote/unquote "Teachers",
Is how much human input
in blue, is needed
To make the method successful
for supervised learning,
Which is what most of
deep learning successes come from

[26:00]
Or most of the data is annotated
by human beings,
The human is at the core of the success.
Most of the data,
that's part of the training
Needs to be annotated by human beings.
With some additional successes,
coming from augmentation methods,

[26:16]
That extend that...
Extend the data, based on which
these networks are trained
And the semi-supervised
reinforcement learning,
And unsupervised methods,

[26:30]
That we'll talk about,
later in the course,
That's where the near-term successes
we hope are.
And with the unsupervised learning approaches,
that's where, the true excitement,
About the possibilities
of artificial intelligence lie.
Being able to make sense, of our world

[26:47]
With minimal input from humans,...
So, we can think of two kinds of
deep learning impact spaces.

[27:00]
One is a special purpose intelligence.
It's taking a problem,
formalizing it.
Collecting enough data on it,
and being able to,
Solve a particular case,
that provides value.

[27:16]
Of particular interest here
is a network
That estimates apartment costs
in the Boston area.
So you could take the
number of bedrooms,
The square feet, and the neighborhood...
And provide as output, the estimated cost.
On the right is the actual data,

[27:32]
Of apartment cost.
We're actually standing,
In an area, that has over 3000 dollars
for a studio apartment
Some of you may be feeling that pain.

[27:47]
And then there's general-purpose intelligence.
Or something that feels like...
Approaching general-purpose intelligence.
Which is reinforcement,
and unsupervised learning.
Here with Andrej, from Andrej Karpathy's,
Pong to Pixels.

[28:03]
A system that takes in,
80 by 80 pixel image
And with no other information is able to beat,
Is able to win at this game.
No information except
a sequence of images,
Raw sensory information,

[28:15]
The same way,
the same kind of information,
That human beings take in, from the visual
Audio, touch, sensory data.
The very low-level data,
and be able to learn to win.
And it's very simplistic,
And it's very artificially constructed world,

[28:31]
But nevertheless,
A world where no feature learning
is performed.
Only raw sensory information
is used to win.
With very sparse minimal human input.
We'll talk about that on Wednesday.

[28:46]
With deep reinforcement learning.
So. But for now we'll focus
on supervised learning.
Where there is input data,
There is a network we're trying to train,
A learning system,
and there's a correct output,

[29:01]
That's labeled by human beings.
That's the general training process
for a neural network.
Input data, labels...
And the training of that network ,
that model.
So that, in a testing stage,

[29:15]
A new input data,
that has never seen before,
It's tasked with producing guesses,
and is evaluated based on that.
For autonomous vehicles,
that means being released
Either in simulation,
or in the real world, to operate.

[29:32]
And how they learn,
how neural networks learn,
Is given, the forward pass,
Of taking the input data,
whether it's from the training stage
In the training stage,
taking the input data,
Producing a prediction.

[29:46]
And then given that
there's ground truth in the training stage,
We can have a measure of error,
based on a loss function.
That then punishes...
The synapses, the connections,
the parameters,
That were involved with making
that wrong prediction.

[30:07]
And it back propagates the error,
through those weights.
We'll discuss that in a little bit
more detail, in a bit here...
So what can we do with deep learning?

[30:16]
You can do one-to-one mapping.
Really you can think of input
as being anything,
It can be a number, a vector of number,
a sequence of numbers
A sequence of vector of numbers...
Anything you can think of,
from images to video,
To audio, to text
can be represented in this way.

[30:31]
And the output can, the same,
be a single number,
Or it can be images,
video, text, audio.
One-to-one mapping on the bottom,
One-to-many, many-to-one,
many to many, and...
Many to many with different
starting points for the data.

[30:49]
Asynchronous.
Some quick terms, that will come up
Deep learning is the same as
neural networks,
It's really deep neural networks,
large neural networks

[31:03]
It's a subset of machine learning,
that has been
Extremely successful in the past decade.
Multi-layer perceptron,
deep neural network ,
Recurrent neural network
Long short-term memory network
LSTM

[31:17]
Convolution neural network and
deep belief networks,
All of these will come up
to the slides...
And, there is specific operations,
Layers within these networks of
Convolution, pooling, activation,
and back propagation.

[31:31]
This concept that we'll discuss,
In this class.
Activation functions,
there's a lot of variants.
On the left is the activation function,
the left column,
And the x-axis is the input,

[31:46]
On the y-axis is the output.
The sigmoid function,
the output.
If the font is too small,
the output is...
Not centered at zero.
For the Tanh function,
it's centered at zero;

[32:02]
But it still suffers from vanishing gradients.
Vanishing gradients is
when the value,
The input is low or high.
The output of the network,
as you see in the right column,

[32:15]
There, the derivative of the function
is very low.
So the learning rate is very low.
For ReLU,
Not, it's also not zero centered,
But it does not suffer from
vanishing gradients.

[32:32]
Back propagation is
the process of learning
It's the way we take goal from error,
Compute as the loss function,
At the bottom right of the slide,
Taking the actual output of the network
with a forward pass,
Subtracting it from the ground truth,

[32:48]
Squaring, dividing by two,
And than using that loss function.
that back propagate,
Through, to construct a gradient,
to back propagate the error.
To the weights that were responsible,
For making either a correct,
or an incorrect decision.

[33:02]
So the subtasks are there,
there's a forward pass,
There's a backward pass,
and...
A fraction of the weight's gradient
subtracted from the weight.
That's it!
That process is modular,
So it's local
to each individual neuron,

[33:17]
Which is why it's extremely,...
We're able to distribute it
across multiple,
Across the GPU.
Parallelize across the GPU.

[33:31]
So, learning for a neural network,
These competition units
are extremely simple.
They're extremely simple to then...
Correct when they make an error,
when they're
Part of a larger network,
that makes an error.
And, all that boils down to,

[33:45]
Is essentially an optimization problem.
Where the objective,
utility, function is
The loss function,
and the goal is to minimize it.
And we have to
update the parameters
The weights, and the synapses,
And the biases to decrease that loss function.

[34:01]
And that loss function is
highly nonlinear.
Depending on the activation function's
different properties,
Different issues arise.
There's vanishing gradients,
for sigmoid.

[34:15]
Where the learning can be slow
There's dying ReLU's...
Where the derivative is exactly zero,
For inputs less than zero.
There are solutions to this,
like leaky ReLU's

[34:30]
And a bunch of details,
you may discover
When you try to win
the deep traffic competition
But, for the most part
These are the main
activation functions
And it's the choice
of the neural network designer

[34:45]
Which one works best...
There's saddle points,
all the problems
From your miracle,
non-linear optimization
That arise, come up here.
It's hard to break symmetry,
And stochastic gradient descent

[35:02]
Wthout any kind of tricks to it,
Can take a very long time,
to arrive at the minima
One of the biggest problems
in all of machine learning
And certainly deep learning,
is overfitting

[35:15]
You can think of the blue dots
and a plot here
As the data, to which we want
to fit a curve
We want to design a learning system
that approximates
The regression of this data.
So, in green, is a sine curve

[35:32]
Simple. Fits well.
And then, there's
a ninth degree polynomial
Which fits even better,
in terms of the error
But it clearly overfits this data
If there's other data

[35:46]
That it has not seen yet
that it has to fit
It's likely to produce a high error
So it's overfitting the training set
This is a big problem
for small data sets
And so we have to fix that,
with regularization

[36:00]
Regularization is a set of methodologies
That prevent overfitting
Learning the training too well,
in order
And then to
not be able to generalize
To the testing stage
And overfitting,
the main symptom

[36:16]
Is the error decreases
in training set
But increases in the test set.
So there's a lot of techniques
and traditional machine learning
That deal with this;
Cross validation, and so on...
But because of the cost of training
for neural networks

[36:31]
Its traditional to use
what's called a validation set
So you create a subset of the training
That you keep away
For which you have
the ground truth
And use that, as a representative
of the testing set.

[36:46]
So you...
Perform early stoppage,
or more realistically
Just save a checkpoint.
Often.
To see how, as the training evolves,
The performance changes
on the validation set,

[37:01]
And so you can stop,
when the performance
In the validation set is
getting a lot worse
It means you're overtraining
on the training set.
In practice, of course,
We run training much longer

[37:15]
And see when,
what is the best performing
What is the best performing
Snapshot checkpoint of the network?
Dropout, is another very powerful
regularization technique.
Where we randomly remove
part of the network

[37:31]
Randomly remove some of the nodes
in the network
Along, with it's incoming
and outgoing edges
So what that really looks like,
Is a probability of keeping a node.
And in many deep learning
frameworks today

[37:45]
It comes with a dropout layer
So it's essentially a probability
That's usually greater than 0.5
That a node will be kept.
For the input layer
The probability should be much higher,
Or, more effectively,
what works well is just adding noise

[38:02]
What's the point here?
You want to create enough diversity
in the training data
Such that it is generalizable,
to the testing.
And as you'll see with
deep traffic competition,

[38:15]
There's L2 and L1 penalty,
Weight decay, weight penalty
Where, there's a penalisation
on the weights that get too large
The L2 penalty keeps the weight small
Unless the error derivative is huge

[38:30]
And produces a smoother model,
And prefers to distribute
When there is two similar inputs
It prefers to put
half the weights on each
Distribute the weights
As opposed to putting the weight
on one of the edges.

[38:45]
Makes the network more robust
L1 penalty has the one benefit
That, for really large weights
They're allowed to be, to stay.
So it allows for a few weights
to remain very large.
These are the regularization techniques
And I wanted to mention them
because they're useful

[39:01]
To some of the competitions,
here in the course.
And I recommend to go to
playground
To tensorflow playground
To play around with
some of these parameters
Where you get to,
online in the browser
Play around with different inputs,
different features

[39:17]
Different number of layers,
and regularization techniques
And to build your intuition
about classification
Regression problems,
given different input data sets.
So what changed? Why over
the past many decades

[39:34]
Neural networks that have gone
through two winters
Are now again
Dominating the artificial intelligence
community
CPUs, GPUs, ASICs,

[39:45]
So, computational power
has skyrocketed
From Moore's law to GPUs
There is huge data set,
including ImageNet, and others
There is research;
Back propagation

[40:02]
In the 80's,
The convolutional neural networks
LSTMs, there's been a lot of
interesting breakthroughs
About how to design these architectures
How to build them, such that
they're trainable efficiently

[40:17]
Using GPUs.
There is the software infrastructure
From being able to share the data,
or get;
To being able to train networks,
and share code
And effectively view neural networks
as a stack of layers

[40:33]
As opposed to having to
implement stuff from scratch
With TensorFlow, PyTorch and
other deep learning frameworks
And there's huge financial backing
from Google, Facebook, and so on...

[40:46]
Deep learning...
..is...
In order to understand,
why it works so well
And where it's limitations are...
We need to understand
where our own intuition comes from

[41:00]
About what is hard,
and what is easy
The important thing
about computer vision
Which is a lot of
what this course is about
Even in deep reinforcement
learning formulation
Is that visual perception
for us human beings
Was formed 540 million years ago

[41:17]
That's 540 million years
worth of data
An abstract thought
Is only formed about
a 100 thousand years ago
That's several orders of magnitude less data

[41:32]
So we can make,
with the neural networks
Predictions that seemed trivial
Trivial to us human beings
But completely challenging and wrong
to neural networks

[41:48]
Here, on the left,
showing a prediction of a dog
With a little bit of a distortion and noise
added to the image
Producing the image on the right
And your network is confidently
99 percent plus accuracy,
Predicting that it's an ostrich

[42:05]
And there's all these problems
to deal with
Whether it's in computer vision data,
Whether it's in text data, audio...
All of this variation arises

[42:15]
In vision,
It's illumination variability
The set of pixels and the numbers
look completely different
Depending on the lighting conditions
It's the biggest problem in driving
Is, lighting conditions,
lighting variability.
Pose variation
Objects need to be learned from
every different perspective

[42:33]
I'll discuss that for
when sensing the driver
Most of....
Most of the deep learning work
that's done in the face
On the human,
is done on the frontal face
Or semi frontal face.
There's very little work done
on the full 360 pose

[42:50]
Variability that a human being
could take on.
Intraclass variability
for the classification problem,
For the detection problem...
There is a lot of different
kinds of objects

[43:02]
For cats, dogs, cars,
bicyclists, pedestrians.
So that brings us
to object classification.
And I'd like to take you through
where deep learning
Has taken big strides
for the past several years

[43:16]
Leading up to this year, to 2018
So let's start at
object classification
Is when you take a single image,
And you have to say...
One class, that's most likely
to belong in that image.

[43:32]
The most famous variant of that
is the ImageNet competition
ImageNet challenge.
ImageNet data set is a data set
of 14 million images
With 21,000 categories
And...
For, say, the category of fruit

[43:46]
There's a total of
188,000 images of fruit
And there is 1200 images
of Granny Smith apples.
It gives you a sense, of what
we're talking about here
So this has been, the source

[44:00]
Of a lot of interesting breakthroughs
in deep learning
And a lot of the excitement,
in deep learning
It's first,
the big successful network
At least, one that became famous
In deep learning is AlexNet in 2012

[44:17]
That took a leap of...
A significant leap in performance
on the ImageNet challenge.
So it was one of the first neural networks
That was successfully trained
on the GPU
And achieved an incredible
performance boost

[44:30]
Over the previous year
on the ImageNet challenge.
The challenge is:
...and I'll talk about some of these networks...
It's to given a single image,
give five guesses,
And you have five guesses to guess
For one of them to be correct

[44:46]
The human annotation is a question
often comes up
So how do you know the ground truth?
Human level performance is
5.1 percent accuracy, on this task.
But, the way the annotation
for ImageNet is performed, is

[45:01]
There's a Google search,
where you pull the images
Already labeled for you,
and then the annotation that
Mechanical Turk, other humans perform
Is just binary:
Is this a cat, or not a cat
So they're not tasked with performing

[45:15]
The very high-resolution semantic
labeling of the image.
Okay. So, through,
from 2012 with AlexNet, to today
And the big transition in 2018
of the ImageNet challenge

[45:31]
Leaving Stanford and going to Kaggle.
It's sort of a monumental step
Because in 2015 with the ResNet network
Was the first time
That the human level performance
was exceeded
And I think this is,
a very important

[45:51]
Map of where deep learning is.
For particularly what I would argue
is a toy example
Despite the fact that it's 14 million images
So we're developing
state-of-the-art techniques here

[46:02]
And in next stage,
as we are now exceeding
Human level performance, on this task
Is how to take these methods
into the real world.
To perform scene perception,
to perform driver state perception.

[46:18]
In 2016, and 2017
CUImage and SENnet has
a very unique new addition
To the previous formulations
that has achieved
An accuracy of 2.2 percent error

[46:32]
2.25 percent error on the
ImageNet classification challenge.
It's an incredible result.
Ok, so you have this
image classification architecture
That takes in a single image,
and produces convolution

[46:45]
And takes it through pooling convolution,
and at the end
Fully connected layers and performs
A classification task,
or regression task.
And you can swap out that layer
to perform any kind of other task
Including with
recurrent neural networks of

[47:01]
Image captioning, and so on...
Or localization of bounding boxes
Or, you can do
fully convolutional networks
Which we'll talk about on Thursday
Which is when you take an image
as an input,

[47:16]
And produce an image as an output.
But where the output image,
in this case,is a segmentation.
Is, where a color indicates
what the object is.
The category of the object.
So it's pixel level segmentation,

[47:30]
Every single pixel in the image
is assigned,
A class, a category,
where that pixel belongs to.
This is, the kind of task,
That's overlaid on top of
other sensory information,

[47:45]
Coming for the car in order to
Perceive the external environment
You can continue to extract information
From images in this way
To produce image to image mapping
For example to colorize images
And take from grayscale images
to color images

[48:04]
Or you can use that kind of
heat map information
To localize objects in the image
So as opposed to just classifying
that this is an image of a cow
R-CNN, Fast and Faster R-CNN,

[48:16]
And a lot of other localization networks
Allow you to propose different candidates
For where exactly the cow
is located in this image
And thereby being able
to perform object detection
Not just object classification.

[48:32]
In 2017 there has been
a lot of cool applications
Of these architectures
One of which is background removal
Again mapping from image to image
Ability to remove
background from selfies

[48:46]
Of humans or human-like
pictures of faces
The reference is,
with some incredible animations,
Are in the bottom of the slide,
And the slides are now available online

[49:04]
Pix2pixHD
There's been a lot of work in GANs
In Generative Adversarial Networks
In particular in driving

[49:16]
GANs have been used to generate examples
That generate examples
from source data
Whether that's from raw data
Or in this case with pix2pixHD
Is taking coarse semantic labeling
of the images

[49:33]
Pixel level, and producing
Photorealistic, high-definition
images of the forward roadway
This is an exciting possibility
For being able to generate

[49:45]
A variety of cases
for self-driving cars
For autonomous vehicles
to be able to learn
To generate, to augment the data
And be able to change the way
different roads look
Road conditions,
To change the way vehicles look
cyclists, pedestrians.

[50:00]
Then we can move on to
recurrent neural networks
Everything I've talked about
was one-to-one mapping
From image to image,
or image to number
Recurrent neural networks
work with sequences
We can use sequences
to generate handwriting

[50:18]
To generate text captions
from an image
Based on the localization,
as the various detections, in that image.
We can provide
video description generation

[50:32]
So taking a video
And combining
convolutional neural networks
With recurrent neural networks
Using convolutional neural networks
to extract features
Frame to frame
And using those extracted features
To input into our RDRN ends,
to then generate labeling

[50:50]
A description of what's
going on in the video
A lot of exciting approaches
for autonomous systems
Especially in drones
Where the time
to make a decision is short

[51:04]
Same with the RC car
traveling 30 miles an hour
Attentional mechanisms
For steering the attention
of the network
Have been very popular
For the localization tasks
and for just saving

[51:16]
How much interpretation of the image
How many pixels
need to be considered
In the classification task
So we can steer,
we can model the way
A human being looks
around an image
To interpret it
And use the network
to do the same.

[51:30]
And we can use
that kind of steering
To draw images, as well.
Finally the big breakthroughs in 2017
Came from this Pong to Pixels

[51:48]
The reinforcement learning
using sensory data
Raw sensory data
And use reinforcement learning methods
Deep are all methods of which
we'll talk about on Wednesday
I'm really excited about...
The underlying methodology of
deep traffic, and deep crash

[52:03]
Is using neural networks
as the approximators
Inside reinforcement learning approaches.
So AlphaGo in 2016,
have achieved
a monumental task.

[52:16]
That when I first started
in artificial intelligence
Was told to me is impossible
for a system to accomplish
Which is to win at the game of Go
Against the top human player
in the world.
However that method was trained
on human expert positions

[52:34]
The Alphago system,
was trained on previous games
Played by human experts.
And in an incredible accomplishment
AlphaGo Zero in 2017

[52:46]
Was able to beat AlphaGo,
And many of it's variants
By playing itself,
from zero information
So no knowledge of human experts

[53:01]
No games, no training data
very little human input
And what more, it was able to generate
Moves, that were surprising
to human experts.

[53:15]
I think it's Einstein
that said that intelligence
That the key mark of intelligence
is imagination.
I think it's beautiful to see
an artificial intelligence system
Come up with something
that surprises human experts

[53:31]
Truly surprises...
For the gambling junkies,
DeepStack
And a few other variants
Have been used in 2017
to win a heads-up poker.

[53:45]
Again another incredible result!
I was always told an artificial intelligence
would be impossible
For Deep,
For any machine learning method
to achieve
And was able to beat
a professional player
And several competitors
have come along since

[54:00]
We're yet to be able to beat
To win, in a tournament setting,
so multiple players
For those unfamiliar
heads-up poker is one-on-one.
It's a much much smaller,
easier space to solve.
There's a lot more human-to-human
dynamics going on,

[54:15]
For when there's multiple players.
But that's the task for 2018
And the drawbacks!
It's one of my favorite videos
I show it often, of Coast runners.
For these deep reinforcement learning
approaches

[54:32]
The learning of the reward function
The definition of the reward function
Controls how
the actual system behaves
And this will come...
This would be extremely important for us,
with autonomous vehicles

[54:48]
Here the boat is tasked with
Gaining the highest number of points,
And it figures out that it
does not need to race,
Which is the whole point
of the game,
In order to gain points
But instead, pick up green circles

[55:02]
That regenerate themselves,
over and over.
This is the...
The counterintuitive behavior
of a system
That would not be expected

[55:15]
When you first designed
the reward function
And this is
a very formal simple system
Nevertheless
Is extremely difficult to come up
with a reward function
That makes it operate in the way
you expect it to operate
Very applicable for autonomous vehicles

[55:31]
Of course in the perception side
As I and mentioned with
the ostrich and the dog
A little bit of noise,
with 99.6 percent confidence
We can predict
That the noise up top is
a robbing, a cheetah,
Armadillo, lesser Panda...

[55:45]
These are outputs from actual
state-of-the-art neural networks
Taking in the noise, and
producing a confident prediction
It should build our intuition,
to understand that we don't
That the visual characteristics,

[56:01]
The spatial characteristics of an image
Did not necessarily convey
the level of hierarchy
Necessary to function in this world.
In a similar way,
with a dog and the ostrich

[56:15]
And everything and an ostrich
Network confidently,
with a little bit of noise
Can make the wrong prediction
Thinking that school bus,
is an ostrich
And a speaker is an ostrich
They're easily fooled
But not really...

[56:33]
Because they perform the task
that they were trained to do, well
So we have to make sure
we keep our intuition
Optimized to the way machines learn

[56:47]
Not the way humans have learned
Over the 540 million years of data
That we've gained
Through developing the eye
through evolution
The current challenges
we're taking on
First: Transfer learning

[57:00]
There's a lot of success
in transfer learning
Between domains that are
very close to each other
So, image classification
from one domain to the next.
There's a lot of value
in forming representations
Of the way scenes look,
in order
Natural scenes look,

[57:16]
In order to do scene segmentation
The driving case, for example.
But we're not able to do
any bigger leaps,
In the way it would perform
transfer learning
The biggest challenge
for deep learning
Is to generalize

[57:31]
Generalize across domains.
It lacks the ability to reason,
In the way that we've defined
understanding previously
Which is the ability to turn
complex information
Into simple useful information.
Convert domain specific,

[57:48]
Complicated sensory information.
That doesn't relate to
the initial training set.
That's the open challenge
for deep learning
Train on very little data,
and then go and reason,
And operate in the real world.

[58:01]
Right now, you'll know,
it's very inefficient
They require big data
They require supervised data
Which means they need human.
Cost a human input
They're not fully automated,
Despite the fact that
the feature learning

[58:16]
Incredibly the big breakthrough
Feature learning is performed
automatically,
You still have to do a lot of design,
Of the actual architecture
of the network
And all the different
hyper parameter tuning
needs to be performed.
Human input
Perhaps a little bit more educated
human input,

[58:32]
A former PhD students,
postdocs faculty
Is required to tune
these hyper parameters.
But nevertheless, human input
is still necessary.
They cannot be left alone.
For the most part...

[58:47]
The reward. Defining the reward
As we saw with coast run
Is extremely difficult
For systems that operate
in the real world
Transparency
Quite possibly it's not
an important one
But neural networks,
currently, are a black box.

[59:01]
For the most part.
They're not able to accept
Through a few successful
visualization methods
That visualize different aspects
of the activations
They're not able to reveal,
to us humans
Why they work,
or where they fail

[59:17]
And this is a philosophical question,
For autonomous vehicles,
That we may not care
as human beings
If a system works well enough.
But I would argue that,
it will be a long time,
Before systems work well enough,

[59:31]
Or we don't care.
We'll care,
And we'll have to work together
with these systems
And that's where transparency,
communication,
...collaboration is critical.
Edge cases.
It's all about edge cases.
In robotics, in autonomous vehicles...

[59:47]
The 99.9 percent of driving
is really boring,
It's the same.
Especially highway driving.
Traffic driving. It's the same.
The obstacle avoidance,
the car following the lanes...
...centering.
All these problems are trivial.

[1:00:01]
It's the edge cases.
Trillions of edge cases,
They need to be generalised over,
On a very small amount
of training data.

[1:00:15]
So again I return to:
Why deep learning?
I mentioned a bunch of challenges,
And this is an opportunity!
It's an opportunity,
to come up with techniques,

[1:00:33]
that operate successfully in this world.
So I hope the competitions
we present in this class,
And the autonomous vehicle domain,
Will give you some insight,
and an opportunity to apply...
In some of these cases are
open research problems,

[1:00:47]
Wth semantic segmentation
of external perception,
With control of the vehicle,
and deep traffic
And, with deep crash,
Of control of the vehicle,
and under actuated

[1:01:01]
High speed conditions,
and the driver state perception.
So with that, I wanted to introduce
deep learning to you today,
Before we get to the fun tomorrow
of autonomous vehicles.

[1:01:17]
So, I would like to thank:
Nvidia, Google, Autoliv,
Toyota. And, at the risk of
setting off people's phones:
Amazon Alexa, Auto...

[1:01:31]
But, truly, I would like to say,
that I've been humbled
Over the past year, by the
thousands of messages
were received
By the attention.
By the 18,000 competition entries.

[1:01:47]
By the many people across the world,
not just here at MIT,
That are brilliant,
that I got a chance to interact with.
And I hope we go bigger,
And do some impressive stuff in 2018.

[1:02:00]
Thank you very much,
and tomorrow is self-driving!