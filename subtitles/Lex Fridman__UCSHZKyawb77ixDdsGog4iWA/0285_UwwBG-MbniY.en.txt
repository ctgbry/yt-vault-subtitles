[00:00]
The following is a conversation with
Daniel Conorman, winner of the Nobel
Prize in Economics for his integration
of economic science with the psychology
of human behavior, judgment, and
decision-making. He's the author of the
popular book thinking fast and slow that

[00:16]
popular book thinking fast and slow that
summarizes in an accessible way his
research of several decades, often in
collaboration with Amos Diverski on
cognitive biases, prospect theory, and
happiness. The central thesis of this
work is the dichotomy between two modes

[00:31]
work is the dichotomy between two modes
of thought. What he calls system one is
fast, instinctive and emotional. System
two is slower, more deliberative and
more logical. The book delineates
cognitive biases associated with each of
these two types of thinking. His study

[00:47]
these two types of thinking. His study
of the human mind and its peculiar and
fascinating limitations are both
instructive and inspiring for those of
us seeking to engineer intelligence
systems. This is the artificial
intelligence podcast. If you enjoy it,

[01:01]
intelligence podcast. If you enjoy it,
subscribe on YouTube, give it five stars
on Apple Podcast, follow on Spotify,
support it on Patreon, or simply connect
with me on Twitter, Lex Freedman,
spelled F R I D M. I recently started
doing ads at the end of the

[01:16]
doing ads at the end of the
introduction. I'll do one or two minutes
after introducing the episode and never
any ads in the middle that can break the
flow of the conversation. I hope that
works for you and doesn't hurt the
listening experience.
This show is presented by Cash App, the

[01:30]
This show is presented by Cash App, the
number one finance app in the app store.
I personally use Cash App to send money
to friends, but you can also use it to
buy, sell, and deposit Bitcoin in just
seconds. Cash App also has a new
investing feature. You can buy fractions
of a stock, say $1 worth, no matter what

[01:46]
of a stock, say $1 worth, no matter what
the stock price is. Broker services are
provided by Cash App Investing, a
subsidiary of Square and member SIPC.
I'm excited to be working with Cash App
to support one of my favorite
organizations called FIRST. Best known

[02:00]
organizations called FIRST. Best known
for their first robotics and Lego
competitions. They educate and inspire
hundreds of thousands of students in
over 110 countries and have a perfect
rating at Charity Navigator, which means
that donated money is used to maximum
effectiveness. When you get Cash App

[02:16]
effectiveness. When you get Cash App
from the App Store, Google Play, and use
code Lex podcast, you'll get $10 and
Cash App will also donate $10 to First,
which again is an organization that I've
personally seen inspire girls and boys
to dream of engineering a better world.

[02:32]
to dream of engineering a better world.
And now here's my conversation with
Daniel Conorman.
You tell a story of an SS soldier early
in the war, World War II, in uh Nazi
occupied France in Paris where you grew
up. He uh picked you up and hugged you

[02:48]
up. He uh picked you up and hugged you
and showed you a picture of a boy, maybe
not realizing that you were Jewish.
Not maybe, certainly not. So, I told you
I'm from the Soviet Union that was
significantly impacted by the war as

[03:01]
significantly impacted by the war as
well and I'm Jewish as well.
What do you think World War II taught us
about human psychology broadly?
Well, I think the the only big surprise
is the extermination policy genocide

[03:18]
is the extermination policy genocide
by the German people. That's when you
look back on it and you know I think
that's a major surprise.
It's a surprise because
it's a surprise that they could do it.

[03:31]
it's a surprise that they could do it.
It's a surprise that they that enough
people willingly participated in that.
This is this is a surprise.
Now it's no longer a surprise but it's
changed
many people's views I think about about

[03:47]
many people's views I think about about
human beings. Uh certainly for me the
Akman trial in that teaches you
something because it's very clear that
if it could happen in Germany it could

[04:00]
if it could happen in Germany it could
happen anywhere. It's not that the
Germans were special. This could happen
anywhere.
So what do you think that is? Do you
think we're all capable of evil? We're
all capable of cruelty.
I don't think in those terms. I think

[04:16]
I don't think in those terms. I think
that what is certainly possible is you
can dehumanize people so that they you
treat them not as people anymore but as
animals and and the same way that you

[04:31]
animals and and the same way that you
can slaughter animals without feeling
much of anything. uh it can the same and
when you feel that the I think the
combination of
dehumanizing the other side and and

[04:47]
dehumanizing the other side and and
having uncontrolled power over other
people I think that doesn't bring out
the most generous aspect of human
nature. So, uh, that Nazi soldier,
uh, you know, he he was a good man. I

[05:03]
uh, you know, he he was a good man. I
mean, you know, he and he was perfectly
capable of killing a lot of people, and
I'm sure he did.
But what what did the Jewish people mean
to Nazis? So what the dismissal of

[05:17]
to Nazis? So what the dismissal of
Jewish as
well
worthy of
again this is surprising that it was so
extreme but it's not
one thing in human nature I don't want
to call it evil but the distinction

[05:32]
to call it evil but the distinction
between the in-roup and the outroup that
is very basic so that's built in the the
loyalty and affection towards inroup and
the willingness to dehumanize the group

[05:46]
the willingness to dehumanize the group
that's that is inhuman nature and that's
that's what I think uh
probably didn't need the Holocaust to
teach us that but the Holocaust is is a
very sharp lesson of you know what can

[06:01]
very sharp lesson of you know what can
happen to people
and what what people can do
so the effect of the ingroup and the out
groupoup
you know that it's clear that those were
people you know you could you could
shoot them. You could, you know, they

[06:16]
shoot them. You could, you know, they
were not human. They were not, there was
no empathy or very very little empathy
left.
So occasionally you know there might
have been and and very quickly by the
way uh the empathy disappeared if there

[06:33]
way uh the empathy disappeared if there
was initially and the fact that
everybody around you was doing it
that that completely the group doing it
and everybody shooting Jews I think that

[06:47]
and everybody shooting Jews I think that
that uh makes it permissible. Now, how
much, you know, whether it would it
could happen
in every culture or whether the Germans

[07:01]
in every culture or whether the Germans
were just particularly efficient and and
disciplined so they could get away with
it?
That's a question. It's an interesting
question.
Are these artifacts of history or is it
human nature?
I think that's really human nature. You

[07:16]
I think that's really human nature. You
know, you put some people in a position
of power relative to other people and
and then they become less human. They
become different.
But in general in war, outside of

[07:31]
But in general in war, outside of
concentration camps in World War II, it
seems that war brings out
darker sides of human nature, but also
the beautiful things about human nature.
Well, you know, I mean, what it what it
brings out is the the loyalty among

[07:48]
brings out is the the loyalty among
soldiers. I mean, it brings out the
bonding, male bonding, I think, is a
very real thing that that happens. And
so, and and there is a certain thrill to
friendship and there is certainly a

[08:02]
friendship and there is certainly a
certain thrill to friendship under risk
and to shared risk. And so people have
very profound emotions
up to the point where it gets so
traumatic that uh

[08:15]
traumatic that uh
that little is left. But
so let's talk about psychology a little
bit. Uh in your book thinking fast and
slow
you describe two modes of thought.
system one,
the fast, instinctive and emotional one,

[08:32]
the fast, instinctive and emotional one,
and system two, the slower, deliberate,
logical one. At the risk of asking
Darwin to discuss
theory of evolution, uh, can you
describe distinguishing characteristics

[08:45]
describe distinguishing characteristics
for people who have not read your book
of the two systems?
Well, I mean, the word system is a bit
misleading, but it's at the same time
it's misleading. It's also very useful.
But what I call system one, it's easier

[09:02]
But what I call system one, it's easier
to think of it as as a family of
activities. And primarily the way I
describe it is there are different ways
for ideas to come to mind. And some

[09:15]
for ideas to come to mind. And some
ideas come to mind automatically.
And the example standard example is 2
plus two. And then something happens to
you. and and in other cases you've got
to do something you've got to work in
order to produce the idea and my example

[09:32]
order to produce the idea and my example
I always give the same pair of numbers
is 27* 14 I think
you have to perform some algorithm in
your head some steps
and and it takes time
it's a very different nothing comes to

[09:45]
it's a very different nothing comes to
mind except something comes to mind
which is the algorithm I mean that
you've got to perform and then it's work
and it engages es short-term memory and
it engages executive function and it
makes you incapable of doing other

[10:00]
makes you incapable of doing other
things at the same time. So uh the the
main characteristic of system two is
that there is mental effort involved and
there is a limited capacity for mental
effort whereas system one is effortless
essentially that's the major

[10:15]
essentially that's the major
distinction. So you talk about there,
you know, it's really convenient to talk
about two systems, but you also
mentioned just now and in general that
there's no distinct two systems in the
brain from a neurobiological even from

[10:31]
brain from a neurobiological even from
psychology perspective. But why does it
seem to uh from the experiments you've
conducted there does seem to be
kind of emerging two modes of thinking.

[10:46]
kind of emerging two modes of thinking.
So at some point these kinds of systems
came into
a brain architecture. Maybe mammals
share it. But or do you not think of it
at all in those terms that it's all a

[11:00]
at all in those terms that it's all a
mush and these two things just emerge?
you know, evolutionary theorizing about
this is cheap and and easy. So, it's the
way I think about it is that it's very
clear that animals

[11:16]
clear that animals
uh have have a perceptual system and
that includes an ability to understand
the world
at least to the extent that they can
predict. They can't explain anything but
they can anticipate what's going to

[11:30]
they can anticipate what's going to
happen. And that's a key form of
understanding the world. And
my crude idea is that we what I call
system two uh well system two grew out
of this and you know there is language

[11:47]
of this and you know there is language
and there is the capacity of
manipulating ideas and the capacity of
imagining futures and of imagining
counterfactual things that haven't
happened and and to do conditional

[12:00]
happened and and to do conditional
thinking and there are really a lot of
abilities that without language and
without the the very large brain that we
have compared to others would be
impossible. Uh now system one is more

[12:15]
impossible. Uh now system one is more
like what the animals have but system
one uh also can talk. I mean it has
language it understands language indeed
it speaks for us. I mean you know I'm
not choosing every word as a deliberate
process. the words I have some idea and

[12:32]
process. the words I have some idea and
then the words come out and that's
automatic and effortless
and uh many of the experiments you've
done is to show that listen system one
exists and it does speak for us and we
should be careful about it the voice it

[12:47]
should be careful about it the voice it
provides
because I mean you know we have to trust
it u because
it's the speed at which it acts at
system two if we if We're dependent on

[13:00]
system two if we if We're dependent on
system two for survival. We wouldn't
survive very long because it's very
slow.
Yeah. Crossing the street.
Crossing the street. I mean many things
depend on there being automatic. One
very important aspect of system one is
that it's not instinctive. You use the

[13:16]
that it's not instinctive. You use the
word instinctive. It contains skills
that clearly have been learned. So that
skilled behavior like driving a car or
or speaking in fact uh skilled behavior
has to be learned and so it doesn't you

[13:33]
has to be learned and so it doesn't you
know you don't come equipped with with
driving. You have to learn how to drive
and and you have to go through a period
where driving is not automatic before it
becomes automatic. So yeah, you

[13:47]
becomes automatic. So yeah, you
construct I mean this is where you talk
about heristic and biases is you uh to
make it automatic,
you create a pattern and then uh system
one essentially matches a new experience

[14:00]
one essentially matches a new experience
against a previously seen pattern. And
when that match is not a good one,
that's when the cogn all the all the
mess happens. But it's most of the time
it works. And so it's pretty
most of the time the anticipation of
what's going to happen next is correct
and and most of the time uh the plan

[14:18]
and and most of the time uh the plan
about what you have to do is correct and
so most of the time everything works
just fine. What's interesting actually
is that in some sense system one is much

[14:30]
is that in some sense system one is much
better as at what it does than system
two is at what it does. that is there is
that quality of effortlessly solving
enormously complicated problems which
clearly uh exists so that a chess player

[14:46]
clearly uh exists so that a chess player
a very good chess player uh all the
moves that come to their mind are strong
moves. So all the selection of strong
moves happens unconsciously and
automatically and very very fast and and

[15:01]
automatically and very very fast and and
all that is in system one. So system two
verifies.
So along this line of thinking really
what we are are machines that construct
pretty effective system one.

[15:16]
pretty effective system one.
You could think of it that way. So, so
we're now talking about humans, but if
we think about building
artificial intelligence systems, robots,
do you think all the features and bugs
that you have highlighted in human

[15:30]
that you have highlighted in human
beings are useful for constructing AI
systems? So, both systems are useful for
perhaps
instilling in robots.
What is happening these days is that
actually what is happening in deep

[15:47]
actually what is happening in deep
learning is is more like a system one
product than like a system two product.
I mean deep learning matches patterns
and anticipate what's going to happen.
So it's highly predictive.

[16:01]
So it's highly predictive.
uh what
that's right
what deep learning doesn't have and you
know many people think that this is a
critical it it doesn't have the ability
to reason so it it does there is no
system to there but I think very

[16:16]
system to there but I think very
importantly it doesn't have any
causality or any way to represent
meaning and to represent real
interaction so uh until that is solved
uh the you know what can be accomplished

[16:31]
uh the you know what can be accomplished
is marvelous and very exciting but
limited.
That's actually really nice to think of
uh current advances in machine learning
is essentially system one advances. So
how far can we get with just system one

[16:45]
how far can we get with just system one
if we think of deep learning and
artificial intelligence systems in
you know it's very clear that deep mind
has already gone way way beyond what
people thought was possible. I think I
think the thing that has impressed me
most about the developments in AI is the

[17:03]
most about the developments in AI is the
speed. It's that things at least in the
context of deep learning and maybe this
is about to slow down but things moved a
lot faster than anticipated.

[17:15]
lot faster than anticipated.
The transition from solving solving
chess to solving go uh was I mean that's
bewildering how quickly it went.
The move from alpha go to alpha zero is
sort of bewildering the speed at which

[17:31]
sort of bewildering the speed at which
they accomplish that. Now clearly
uh there there so there are many problem
that you can solve that way but there
are some problems for which you need
something else
something like reasoning. well reasoning

[17:46]
something like reasoning. well reasoning
and also you know the one of the real
mysteries uh psychologist Gary Marcus
who is also a critic of AI um I mean he
what he points out and I think he has a

[18:02]
what he points out and I think he has a
point is that uh humans learn quickly
uh children don't need a million
examples they need two or three examples

[18:15]
examples they need two or three examples
So clearly there is a fundamental
difference and what enables
uh what enables a machine to to learn
quickly what you have to build into the
machine because it's clear that you have
to build some expectations or something

[18:31]
to build some expectations or something
in the machine to make it ready to learn
quickly. Uh that's that at the moment
seems to be unsolved. I'm pretty sure
that Deep Mind is working on it, but um
yeah, they're

[18:45]
yeah, they're
if they have solved it, I I haven't
heard yet.
They're trying to actually them and
OpenAI are trying to to start to get to
use neural networks to reason. So,
assemble knowledge.
Uh of course, causality is temporal

[19:01]
Uh of course, causality is temporal
causality is out of reach to most
everybody. You mentioned the benefits of
system one is essentially that it's fast
allows us to function in the world
fast and skilled you know
it's skill
and it has a model of the world you know

[19:16]
and it has a model of the world you know
in a sense I mean there was the early
phase of of uh AI attempted to model
reasoning and they were moderately
successful but you know reasoning by
itself doesn't get you much uh deep

[19:34]
itself doesn't get you much uh deep
learning has been much more successful
in terms of you know what they can do
but now
it's an interesting question whether
it's approaching its limits what do you
think

[19:45]
think
I think absolutely so I I just talked to
Gian Lun he mentioned you know
I know him
so he thinks that uh the limits we're
not going to hit the limits with neol
networks that ultimately this kind of
system one pattern matching will start

[20:01]
system one pattern matching will start
to start to look like system too with
without significant transformation of
the architecture. So I'm more with the
with the majority of the people who
think that yes neural networks will hit

[20:15]
think that yes neural networks will hit
a limit in their capability. He on the
one hand I have heard him tell the
Misabies essentially that you know what
they have accomplished is not a big deal
that they have just touched that
basically you know they can't do

[20:30]
basically you know they can't do
unsupervised learning in in an effective
way and
but you're telling me that he thinks
that the current within the current
architecture you can do causality and
reasoning. So he's very much a
pragmatist in a sense that's saying that

[20:46]
pragmatist in a sense that's saying that
we're very far away that there's still
Yeah. I think uh there's this idea that
he says is uh we can only see one or two
mountain peaks ahead and there might be
either a few more after or thousands

[21:00]
either a few more after or thousands
more after. Yeah.
So that kind of idea.
I heard that metaphor. Yeah.
Right. But nevertheless, it doesn't see
a
the final answer not fundamentally
looking like one that we currently have.

[21:15]
looking like one that we currently have.
So neural networks being a huge part of
that.
Yeah, I mean that's very likely because
because pattern matching is so much of
what's going on. But
and you can think of neural networks as
processing information sequentially.

[21:30]
processing information sequentially.
Yeah. I mean, you know, there is
there is an important aspect to, for
example, you get systems that translate
and they do a very good job, but they
really don't know what they're talking
about. Uh and and and for that I'm

[21:48]
about. Uh and and and for that I'm
really quite surprised for that you
would need you would need an AI that has
sensation an AI that is in touch with
the world.
Yes. uh self awareness and maybe even

[22:01]
Yes. uh self awareness and maybe even
something resembles consciousness kind
of ideas.
Certainly awareness of you know
awareness of what's going on so that the
the words have meaning or can get are in
touch with some perception or some

[22:15]
touch with some perception or some
action.
Yeah. So uh that's a big thing for Yan
is uh what he refers to as grounding to
the physical space. So
So that's what we're talking about the
same.
Yeah. So, but so how how you ground
I mean the grounding without grounding

[22:32]
I mean the grounding without grounding
then you get you get a machine that
doesn't know what it's talking about
because it is talking about the world
ultimately
the question the open question is what
it means to ground I mean we're very uh

[22:45]
it means to ground I mean we're very uh
humanentric in our thinking but what
does it mean for a machine to understand
what it means to be in this world
does it need to have a body does it need
to have a finite tightness like we
humans have all of these elements. It's
it's a very it's

[23:02]
it's a very it's
um you know I'm not sure about having a
body but having a perceptual system
having a body would be very helpful too.
I mean if if you think about human
mimicking human or but having a
perception that seems to be essential

[23:18]
perception that seems to be essential
uh so that you can build you can
accumulate knowledge about the world. So
if a you can im you can imagine a human
completely paralyzed and there is a lot
that the human brain could learn you

[23:32]
that the human brain could learn you
know with a paralyzed body. So uh if we
got a machine that could do that that
would be a big deal.
And then the flip side of that something
you see in children and something in
machine learning world is called active

[23:46]
machine learning world is called active
learning. Maybe it is also
is uh being able to play with the world.
Uh how important for developing system
one or or system two do you think it is
to play with the world to be able to

[24:00]
to play with the world to be able to
interact with?
Certainly a lot a lot of what you learn
as you learn to anticipate
uh the outcomes of your actions. I mean
you can see that how babies learn it you
know with their hands. they how they
learn uh you know to connect uh you know

[24:17]
learn uh you know to connect uh you know
the movements of their hands with
something that clearly is something that
happens in the brain and and and the
ability of the brain to learn new
patterns. So, you know, it's the kind of
thing that you get with artificial limbs

[24:31]
thing that you get with artificial limbs
that you connect it and then people
learn to operate the artificial limb,
you know, really impressively quickly at
least from from what I hear. Uh, so we

[24:45]
least from from what I hear. Uh, so we
have a system that is ready to learn the
world through action.
At the risk of going into way too
mysterious of land, what do you think it
takes to build a system like that?
Obviously,
we're very far from understanding how

[25:00]
we're very far from understanding how
the the brain works, but
how difficult is it to build this
mind of ours?
You know, I mean, I think that Yandun's
answer that we don't know how many
mountains there are. I think that's a

[25:15]
mountains there are. I think that's a
very good answer. I think that, you
know, if you if you look at what Ray
Kotzwell is saying, that strikes me as
offthe-wall, but uh but I think people
are much more realistic than that were
actually Demis is and Jan is and so the

[25:34]
actually Demis is and Jan is and so the
people who are actually doing the work
fairly realistic. I think
to maybe phrase it another way from a
perspective not of building it but from
understanding it.

[25:46]
understanding it.
How complicated are human beings in the
in the following sense.
You know I work with autonomous vehicles
and pedestrians. So we tried to model
pedestrians.
How difficult is it to model a human

[26:00]
How difficult is it to model a human
being,
their perception of the world, the two
systems they operate under sufficiently
to be able to predict whether the
pedestrian is going to cross the road or
not? I'm, you know, I'm fairly
optimistic about that actually because

[26:16]
optimistic about that actually because
what we're talking about is uh a huge
amount of information that every vehicle
has and that feeds into one system into
one gigantic system. And so anything

[26:30]
one gigantic system. And so anything
that any vehicle learns becomes part of
what the whole system knows. And with
with a system multiplier like that uh
there is a lot that you can do. So human
beings are very complicated but and and

[26:45]
beings are very complicated but and and
you know system is going to make
mistakes but human makes mistakes. I
think that they'll be able to I think
they are able to anticipate pedestrians
otherwise a lot would happen. they're
able to,

[27:01]
able to,
you know, they're able to get into a
roundabout and into the into traffic.
So, they must know both to expect or to
anticipate how people will react when
they're sneaking in. And there's a lot

[27:17]
they're sneaking in. And there's a lot
of learning that's involved in that.
Currently, the pedestrians are treated
as things that cannot be hit and they're
not treated as agents with whom you

[27:31]
not treated as agents with whom you
interact in a game theoretic way. So,
I mean, it's not it's a totally open
problem and every time somebody tries to
solve it, it seems to be harder than we
think. and nobody's really tried to
seriously solve the problem of that

[27:46]
seriously solve the problem of that
dance because uh I'm not sure if you've
thought about the problem of pedestrians
but you're really putting your life in
the hands of the driver. You know there
is a dance there is part of the dance
that would be quite complicated but for

[28:01]
that would be quite complicated but for
example when I cross the street and
there is a vehicle approaching I look
the driver in the eye and I think many
people do that and you know that's a
signal uh that that I'm sending and I
would be sending that machine to an

[28:16]
would be sending that machine to an
autonomous vehicle and it had better
understand it because it means I'm
crossing. So, and there's another thing
you do that actually. So, I'll tell you
what you do because we watched I've
watched hundreds of hours of video on

[28:30]
watched hundreds of hours of video on
this is when you step in the street, you
do that before you step in the street.
And when you step in the street, you
actually look away.
Look away.
Yeah.
Uh now, what what is that?
What that's saying is mean you're
trusting that the car who hasn't slowed

[28:46]
trusting that the car who hasn't slowed
down yet will slow down.
Yeah. And you're telling him,
Yeah. I'm committed. I mean this is like
in a game of chicken. So I'm committed
and if I'm committed I'm looking away.
So there is you you just have to stop.

[29:00]
So there is you you just have to stop.
So the question is whether a machine
that observes that needs to understand
mortality
here. I'm not sure that it's got to
understand so much as it's got to
anticipate.

[29:15]
anticipate.
So, and here, but you know, you're
surprising me because
here I would think that maybe you can
anticipate without understanding because
I think this is clearly what's happening

[29:30]
I think this is clearly what's happening
in playing go or in playing chess.
There's a lot of anticipation and there
is zero understanding.
Exactly.
So, uh
I thought that you didn't need a model
of the human.
Yes. and a model of the human mind to

[29:46]
Yes. and a model of the human mind to
avoid hitting pedestrians. But you are
suggesting that actually
we do. Yeah,
you do.
And
then it's then it's a lot harder.
So this is
and I have a followup question to see
where your intuition lies is it seems

[30:01]
where your intuition lies is it seems
that almost every robot human
collaboration system is a lot harder
than people realize. So,
do you think it's possible for robots
and humans to collaborate successfully?

[30:16]
and humans to collaborate successfully?
We we talked a little bit about
semi-autonomous vehicles like in the
Tesla autopilot, but just in tasks in
general,
if you think we talked about current
neural networks being kind of system
one, do you think

[30:32]
one, do you think
uh those same systems can borrow humans
for system two type tasks and
collaborate successfully? Well, I think
that in any system where humans and and

[30:47]
that in any system where humans and and
the machine interact that the human will
be superfluous within a fairly short
time and that is if if the machine is
advanced enough so that it can really
help the human then it may not need the

[31:00]
help the human then it may not need the
human for a long time. Now it would be
very interesting if if there are
problems that for some reason the
machine doesn't cannot solve but that
people could solve then you would have
to build into the machine an ability to

[31:15]
to build into the machine an ability to
recognize
that it is in that kind of problematic
situation
and and to call the human that that
cannot be easy
without understanding that is it's it
must be very difficult to to program a

[31:33]
must be very difficult to to program a
recognition that you are in a
problematic situation without
understanding the problem. But
that's very true. In order to understand

[31:45]
that's very true. In order to understand
the full scope of situations that are
problematic, you almost need to be smart
enough to solve all those problems.
It's not clear to me how much the
machine will need the human.

[32:00]
machine will need the human.
I think the example of chess is very
instructive. I mean there was a time at
which Kasparov was saying that human
machine combinations will beat
everybody. Uh even Stockfish doesn't
need people.
Yeah.
And Alpha Zero certainly doesn't need

[32:15]
And Alpha Zero certainly doesn't need
people.
The question is just like you said, how
many problems are like chess? And how
many problems are the ones where are not
like chess? Where well every problem in
the end is like chess. The question is
how long is that transition period?

[32:30]
how long is that transition period?
I mean, you know, that's that's a
question I would ask you in terms of
I mean, autonomous vehicle just driving
is probably a lot more complicated than
go to solve that.
Yes.
And that's surprising
because it's open. No, I mean, you know,

[32:46]
because it's open. No, I mean, you know,
it wouldn't that's not surprising to me
because the because
that there is a hierarchical
aspect to this which is recognizing a
situation and then within the situation

[33:01]
situation and then within the situation
bringing bringing up the relevant
knowledge and uh and for that
hierarchical type of system to work uh
you need a more complicated system than
we currently have.

[33:16]
we currently have.
A lot of people think because as human
beings, this is probably the the
cognitive biases, they think of driving
as pretty simple because they think of
their own experience. This is actually a
a big problem for a AI researchers or

[33:32]
a big problem for a AI researchers or
people thinking about AI because they
evaluate how hard a particular problem
is based on very limited knowledge
based on how hard it is for them to do
the task.

[33:45]
the task.
Yeah. and then they take for granted.
Maybe you can speak to that because most
people tell me driving is trivial
and and humans in fact are terrible at
driving is what people tell me and I see
humans and humans are actually

[34:00]
humans and humans are actually
incredible at driving and driving is
really terribly difficult.
Yeah. Uh so is that just another element
of the effects that you've described in
your work on the psychology side?

[34:15]
No, I mean I haven't really, you know, I
would say that my research has
contributed nothing to understanding the
ecology and to understanding the
structure of situations and the
complexity of problems. Uh so all all we

[34:32]
complexity of problems. Uh so all all we
know is very clear that
that goal it's endlessly complicated but
it's very constrained. So uh and and in
the real world there are far fewer

[34:45]
the real world there are far fewer
constraints and and many more potential
surprises. So uh
so that's obvious because it's not
always obvious to people right? So when
you think about
well I mean you know people thought that
reasoning was hard and perceiving was

[35:02]
reasoning was hard and perceiving was
easy but you know they quickly learned
that actually
modeling vision was tremendously
complicated and modeling
even proving theorems was relatively
straightforward. to push back on that a

[35:17]
straightforward. to push back on that a
little bit on the quickly part. They
haven't it took several decades to learn
that and most people still haven't
learned that. I mean our intuition of
course AI researchers have but you you

[35:31]
course AI researchers have but you you
drift a little bit outside the specific
AI field. The intuition is still
perception to sol. That's true. I mean
the intuitions the intuitions of the
public haven't changed radically and
they are they are as you said they're

[35:46]
they are they are as you said they're
evaluating the complexity of problems by
how difficult it is for for them to
solve the problems and that's got very
little to do with the complexities of
solving them in AI. How do you think
from the perspective of AI researcher

[36:03]
from the perspective of AI researcher
do we deal with the intuitions of the
public? So in trying to think I mean
arguably
the combination of uh hype investment
and the public intuition is what led to

[36:16]
and the public intuition is what led to
the AI winters. I'm sure that same can
be applied to tech or that the intuition
of the public leads to media hype leads
to companies investing in the tech and

[36:31]
to companies investing in the tech and
then the tech doesn't make the company's
money and then there's a crash. Is there
a way to educate people to fight the
let's call it system one thinking
in general? No. You know, I think that's

[36:47]
in general? No. You know, I think that's
a simple answer. Um, and it's going to
take a long time before the
understanding of
what those systems can do becomes, you

[37:01]
what those systems can do becomes, you
know, part becomes public knowledge. I
and then and the expectations, you know,
there are several aspects that are going
to be very complicated that are

[37:15]
to be very complicated that are
the fact that you have a device that
cannot explain itself is a major major
difficulty and uh and we're already

[37:30]
difficulty and uh and we're already
seeing that. I mean this is this is
really something that is happening. So
it's happening in the judicial system.
So you have uh you have system that are
clearly better at predicting parole
violations than

[37:45]
violations than
uh than judges but uh but they can't
explain their reasoning and so uh people
don't want to trust them.
We uh seem to in system one even use

[38:00]
We uh seem to in system one even use
cues to make judgments about our
environment.
So this explanability point do you think
humans can explain stuff?
No. But themselves uh I mean there is a
very interesting uh aspect of that.

[38:18]
very interesting uh aspect of that.
Humans think they can explain themselves
right? So when you say something and I
ask you why do you believe that then
reasons will occur to you and you will

[38:30]
reasons will occur to you and you will
but actually my own belief is that in
most cases the reasons have very little
to do with why you believe what you
believe. So that the reasons are a story
that that comes to your mind when you
need to explain yourself. But um but but

[38:48]
need to explain yourself. But um but but
people traffic in those explanations. I
mean the human interaction depends on
those shared fictions and and the
stories that people tell themselves. You
just made me actually realize and we'll

[39:00]
just made me actually realize and we'll
talk about stories in a second
that not to be cynical about it but
perhaps there's a whole movement of
people trying to do explainable AI
and really we don't necessarily need to

[39:16]
and really we don't necessarily need to
explain AI doesn't need to explain
itself. It just needs to tell a
convincing story.
Yeah, absolutely. It doesn't necess the
story doesn't necessarily need to uh
reflect the truth as it might it just
needs to be convincing.

[39:31]
needs to be convincing.
There's something to that.
It can you can say exactly the same
thing in a way that's sounds cynical or
doesn't sound cynical. I mean, so but
but the objective brilliant
of having an explanation is is to tell a

[39:46]
of having an explanation is is to tell a
story that will be acceptable to people
and and and for it to be acceptable and
to be robustly acceptable, it has to
have some element of truth.

[40:00]
have some element of truth.
But but the objective is for people to
accept it.
It's quite brilliant actually. Um but so
on the uh on the stories that we tell,
sorry to ask me ask you the question
that most people know the answer to, but

[40:15]
that most people know the answer to, but
uh you talk about two selves in terms of
how life is lived, the experience self
and the remembering self. Can you
describe the distinction between the
two?
Well, sure. I mean the there is an
aspect of uh of life that occasionally

[40:33]
aspect of uh of life that occasionally
you know most of the time we just live
and we have experiences and they're
better and they are worse and it goes on
over time and mostly we forget
everything that happens or we forget
most of what happens. Then occasionally

[40:47]
most of what happens. Then occasionally
you
when something ends or at different
points uh you evaluate the past and you
form a memory and the memory is
schematic. It's not that you can roll a

[41:01]
schematic. It's not that you can roll a
film of an interaction. You construct in
effect the elements of a story about an
about an episode.
So there is the experience and there is
the story that is created about the

[41:16]
the story that is created about the
experience and that's what I call the
remembering. So I I had the image of two
selves. So there is a self that lives
and there is a self that evaluates
life. Now the paradox and the deep

[41:31]
life. Now the paradox and the deep
paradox in that is that um we have one
system or one self that does the living
but the other system uh the remembering
self is all we get to keep and basically

[41:46]
self is all we get to keep and basically
decision making and and everything that
we do is governed by our memories not by
what actually happened. It's it's
governed by by the story that we told
ourselves or by the story that we're
keeping. So that's that's the

[42:02]
keeping. So that's that's the
distinction.
I mean there's a lot of brilliant ideas
about the pursuit of happiness that come
out of that. What are the properties of
happiness which emerge from self?
There are there are properties of how we

[42:16]
There are there are properties of how we
construct stories that are really
important. So uh that I studied a few
but but
a couple are really very striking and
one is that in stories time doesn't

[42:32]
one is that in stories time doesn't
matter. M
there's a sequence of events or there
are highlight or not
and and how long it took you know they
lived happily ever after or and three

[42:45]
lived happily ever after or and three
years later something it time really
doesn't matter and in stories events
matter but time doesn't
that that leads to a very interesting
set of problems because time is all we

[43:03]
set of problems because time is all we
not to live. I mean, you know, time is
the currency of life. Uh, and yet time
is not represented basically in
evaluated memories. So, that that
creates a lot of uh paradoxes that I've

[43:17]
creates a lot of uh paradoxes that I've
thought about.
Yeah. That are fascinating. But if you
were to
give uh advice
on how one lives a happy life
based on such properties, what's the

[43:31]
based on such properties, what's the
optimal?
Well, you know, I gave up I abandoned
happiness research because I couldn't
solve that problem. I couldn't I
couldn't see. Uh and in the first place,
it's very clear that if you do talk in

[43:46]
it's very clear that if you do talk in
terms of those two selves, then that
what makes the remembering self happy
and what makes the experiencing self
happy are different things. And I I
asked the question uh of suppose you're
planning a vacation and you're just told

[44:02]
planning a vacation and you're just told
that at the end of the vacation you'll
get an amnesic drug so you remember
nothing and they'll also destroy all
your photos. So there'll be nothing.
Would you still go to the same vacation?

[44:17]
Would you still go to the same vacation?
And and it's
it turns out we go to vacations in large
part to construct memories not to have
experiences but to construct memories.
And it turns out that the vacation that

[44:30]
And it turns out that the vacation that
you would want for yourself if you knew
you will not remember is probably not
the same vacation that you will want for
yourself if you will remember. So uh I
have no solution to these problems but

[44:45]
have no solution to these problems but
clearly those are big issues
and you've talked about difficult
issues.
You've talked about sort of how many
minutes or hours you spend about the
vacation. It's an interesting way to
think about it because that's how you
really experience the vacation outside

[45:00]
really experience the vacation outside
the being in it. But there's also a
modern I don't know if you think about
this or interact with it. There's a
modern way to uh magnify the remembering
self which is by posting on Instagram,
on Twitter, on social networks. A lot of

[45:17]
on Twitter, on social networks. A lot of
people live life for the picture that
you take that you post somewhere and now
thousands of people share and it
potentially potentially millions and
then you can relive it even much more
than just those minutes. Do you think

[45:30]
than just those minutes. Do you think
about that
magnification much? You know, I'm too
old for social networks. I, you know,
I've never seen Instagram, so
I cannot really speak intelligently
about those things. I'm just too old.

[45:46]
about those things. I'm just too old.
But it's interesting to watch the exact
effects you describe.
It will make a very big difference. I
mean, and it will make it will also make
a difference. And that I don't know
whether
uh
it's clear that in some ways

[46:04]
it's clear that in some ways
the devices that serve us uh supplant
function. So you don't have to remember
phone numbers. You don't have you you
really don't have to know facts. I mean

[46:15]
really don't have to know facts. I mean
the number of conversations I'm involved
with where somebody says well let's look
it up. Uh so it's it's in a way it's
made conversations
well it's it means that it's much less

[46:30]
well it's it means that it's much less
important to know things. You know it
used to be very important to know
things. This is changing. So the
requirements of that that we
have for ourselves and for other people

[46:46]
have for ourselves and for other people
are changing because of all those
supports and because and I have no idea
what Instagram does but it's uh
well I'll tell you
wish I knew
I mean I I wish I could just have the my
remembering self could enjoy this

[47:01]
remembering self could enjoy this
conversation but I'll get to enjoy it
even more by having watch by watching it
and and talking to others it'll be about
100,000 people as scary as this to to
say will listen or watch this right it

[47:15]
say will listen or watch this right it
changes things it changes the experience
of the world that you seek out
experiences which could be shared in
that way it's and I haven't seen it's
it's the same effects that you described
and I don't think the psychology of that
magnification has been described yet

[47:32]
magnification has been described yet
because it's a new world
you know the sharing
There was a per there was a time when
people read books
and uh and and you could assume

[47:46]
and uh and and you could assume
that your friends had read the same
books that you read.
So there was kind of invisible sharing.
There was a lot of sharing going on and
there was a lot of assumed common
knowledge

[48:00]
knowledge
and you know that was built in. I mean
it was obvious that you had read the New
York Times. It was obvious that you had
read the reviews. I mean uh so a lot was
taken for granted that was shared. Uh
and you know when there were when there

[48:17]
and you know when there were when there
were three television channels it was
obvious that you'd seen one of them
probably the same. Uh
so sharing sharing
always was always there. It was just

[48:30]
always was always there. It was just
different.
At the risk of uh inviting mockery from
you, let me say that that I'm
also a fan of Sartra and Kimu and
existentialist philosophers. And um I'm

[48:45]
existentialist philosophers. And um I'm
joking of course about mockery, but
from the perspective of the two selves,
what do you think of the existentialist
philosophy of life? So trying to really
emphasize the experiencing self as the

[49:00]
emphasize the experiencing self as the
proper way to
or the best way to live life.
I don't know enough philosophy to answer
that. But it's not uh you know the
emphasis on on experience is also the

[49:15]
emphasis on on experience is also the
emphasis in Buddhism.
Yeah. Right. That's right. So uh that's
you just have got to to experience
things and and and not to evaluate and
not to pass judgment and not to score

[49:30]
not to pass judgment and not to score
not to keep score. So, uh,
if when you look at the the grand
picture of experience, you think there's
something to that that one one of the
ways to achieve contentment and maybe
even happiness is letting go of any of

[49:47]
even happiness is letting go of any of
the things
any of the procedures of the remembering
self.
Well, yeah. I mean, I think, you know,
if one could imagine a life in which
people don't score themselves.
Mhm. uh it it feels as if that would be

[50:03]
Mhm. uh it it feels as if that would be
a better life as if the self-scoring and
you know how am I doing uh kind of
question uh
is not is not a very happy thing to have

[50:17]
is not is not a very happy thing to have
but I got out of that field because I
couldn't solve that problem
and and that was because my intuition
was that the experiencing self that's
reality

[50:31]
reality
But then it turns out that what people
want for themselves is not experiences.
They want memories and they want a good
story about their life. And so you
cannot have a theory of happiness that
doesn't correspond to what people want
for themselves. And when I when I

[50:48]
for themselves. And when I when I
realized that this this was where things
were going, I really sort of left the
field of research. Do you think there's
something instructive about this
emphasis of reliving memories in

[51:01]
emphasis of reliving memories in
building AI systems? So, currently
artificial intelligence systems
are more like experiencing cells in that
they react to the environment. there's
some pattern formation like uh learning

[51:16]
some pattern formation like uh learning
so on but you really don't construct
memories uh except in reinforcement
learning every once in a while that you
replay over and over.
Yeah, but but you know that would in
principle would not be

[51:30]
principle would not be
Do you think that's useful? Do you think
it's a feature or a bug of human beings
that we uh that we look back?
Oh, I think that's definitely a feature.
It's not a bug. I mean you you have to
look back in order to look forward. So

[51:46]
look back in order to look forward. So
uh without without looking back you
couldn't you couldn't really
intelligently look forward. You're
looking for the echoes of the same kind
of experience in order to predict how
what the future holds.
Yeah. though Victor Frankle in his book

[52:02]
Yeah. though Victor Frankle in his book
Man's Search for Meaning, I'm not sure
if you've read, but describes his
experience at the concentr concentration
camps during World War II as a way to
describe that
finding identifying a purpose in life, a

[52:16]
finding identifying a purpose in life, a
positive purpose in life can save one
from suffering.
First of all, do you connect with the
philosophy that he describes there? M
not really. I mean the So I can I can

[52:32]
not really. I mean the So I can I can
really see that somebody who has that
feeling of purpose and meaning and so on
that that could sustain you. Uh I in
general don't have that feeling and I'm

[52:47]
general don't have that feeling and I'm
pretty sure that if I were in a
concentration camp I'd give up and die,
you know. So he talks he is he is a
survivor.
Yeah.
And you know he survived with that
and um and I'm not sure how essential to

[53:02]
and um and I'm not sure how essential to
survival this sense is but I do know
when I think about myself that I would
have given up at oh this isn't going
anywhere. uh and there is there is a

[53:16]
anywhere. uh and there is there is a
sort of character that that that manages
to survive in conditions like that and
then because they survive they tell
stories and it sounds as if they
survived because of what they were doing
we have no idea they survived because of

[53:32]
we have no idea they survived because of
the kind of people that they are and
they are the kind of people who survives
and who tell themselves stories of a
particular kind so I'm not uh
so you don't think seeking purpose is a
significant driver in

[53:46]
significant driver in
our I mean it's it's a very interesting
question because when you ask people
whether it's very important to have
meaning in their life they say oh yes
that's the most important thing but when
you ask people what kind of a day did

[54:00]
you ask people what kind of a day did
you have and and you know what were the
experiences that you remember you don't
get much meaning you get social
experiences
then uh and

[54:15]
then uh and
and some people say that for example in
in in child you know in taking care of
children the fact that they are your
children and you're taking care of them
uh makes a very big difference. I think

[54:30]
uh makes a very big difference. I think
that's entirely true. uh but it's more
because of a story that we're telling
ourselves which is a very different
story when we're taking care of our
children or when we're taking care of
other things.
Jumping around a little bit in doing a

[54:47]
Jumping around a little bit in doing a
lot of experiments. Let me ask a
question.
Most of the work I do for example is in
in the in the real world but most of the
clean good science that you can do is in
the lab. So that distinction

[55:02]
the lab. So that distinction
do you think we can understand the
fundamentals of human behavior
through controlled experiments in the
lab?
If we talk about pupil diameter for
example, it's much easier to do when you

[55:18]
example, it's much easier to do when you
can control lighting conditions.
Yeah. Right. Both. Uh so when we look at
driving lighting variation destroys
almost completely your ability to use
pupil diameter but in the lab for as I

[55:33]
pupil diameter but in the lab for as I
mentioned semi-autonomous or autonomous
vehicles in driving simulators we can't
we don't
capture true honest uh human behavior in
that particular domain.

[55:46]
that particular domain.
So in your what's your intuition? How
much of human behavior can we study in
this controlled environment of the lab?
A lot. But you'd have to verify it. You
know that you your conclusions are
basically limited to the situation to

[56:03]
basically limited to the situation to
the experimental situation. Then you
have to jump that a big inductive leap
to the real world. Uh so and and that's
the flare. That's where the difference I

[56:16]
the flare. That's where the difference I
think between the good psychologists and
others that are mediocre is in the sense
that your experiment capture something
that's important,
right?
And something that's real and others are

[56:31]
And something that's real and others are
just running experiments.
So what is that like the birth of an
idea to development in your mind to
something that leads to an experiment?
Is that similar to maybe like what
Einstein or a good physicist do is your

[56:45]
Einstein or a good physicist do is your
intuition. You basically use your
intuition to build up.
Yeah. But I mean, you know, it's it's
very skilled intuition.
I mean, I I just had that experience
actually. I had an idea that turns out
to be very good idea uh a couple of days

[57:00]
to be very good idea uh a couple of days
ago and and you and you have a sense of
that building up. So, I'm working with a
collaborator
and he he essentially was saying, you
know, what what are you doing? You know,
what's what's going on? And I was I

[57:15]
what's what's going on? And I was I
really I couldn't exactly explain it,
but I knew this is going somewhere. But,
you know, I've been around that game for
a very long time. And so, I can you you
develop that anticipation that yes, this

[57:30]
develop that anticipation that yes, this
this is worth following up.
That's part of the skill. Is that
something you can reduce to words in
describing a process in in the form of
advice
to others?
No.
Follow your heart essentially.

[57:45]
Follow your heart essentially.
I mean, you know, it's it's like trying
to explain what it's like to drive. It's
not you've got to break it apart and
it's not
and then you lose the experience.
You mentioned collaboration. And you've
written about your collaboration with

[58:02]
written about your collaboration with
Amos Diverski
that this is you writing the 12 or 13
years in which most of our work was
joint or years of interpersonal and
intellectual bliss. Everything was

[58:15]
intellectual bliss. Everything was
interesting. Almost everything was
funny. And there was a current joy of
seeing an idea take shape. So many times
in those years we shared the magical
experience of one of us saying something
which the other one would understand
more deeply than the speaker had done.

[58:30]
more deeply than the speaker had done.
Contrary to the old laws of information
theory, it was common for us to find
that more information was received than
had been sent. I have almost never had
the experience with anyone else. If you
have not had it, you don't know how

[58:45]
have not had it, you don't know how
marvelous collaboration can be.
So let me ask a perhaps a silly
question.
How does one find and create such a
collaboration?
That may be asking like how does one

[59:00]
That may be asking like how does one
find love? But
yeah, you have to be you have to be
lucky
and and and I think you have to have the
character for that because I've had many
collaborations. I mean, none were as
exciting as with Elmos, but I've had and

[59:17]
exciting as with Elmos, but I've had and
I'm having just very So, it's a skill. I
think I'm good at it.
Uh, not everybody is good at it. And
then it's the luck of finding people who

[59:30]
then it's the luck of finding people who
are also good at it.
Is there advice in a form for for a
young scientist
who also seeks to violate this law of
information theory?

[59:48]
I really think it's so much luck is
involved and you know in in those
really serious collaborations at least
in my experience are a very personal

[1:00:03]
in my experience are a very personal
experience and and I have to like the
person I'm working with otherwise you
know I mean there is that kind
a collaboration which is like an
exchange, a commercial exchange of I'm

[1:00:17]
exchange, a commercial exchange of I'm
giving this, you give me that. But the
the real ones are interpersonal. They're
between people who like each other and
and who like making each other think and
who like the way that the other person

[1:00:31]
who like the way that the other person
responds to your thoughts. Uh you have
to be lucky.
Yeah. I mean, but I already noticed the
p even just me showing up here. You
you've quickly started to digging in on
a particular problem I'm working on and

[1:00:46]
a particular problem I'm working on and
already new information started to
emerge. If is that a process just a
process of curiosity of talking to
people about problems and seeing
I'm curious about anything to do with AI
and robotics and you know and uh so and

[1:01:02]
and robotics and you know and uh so and
I knew you were dealing with that so I
was curious. just follow your curiosity.
Jumping around on on the psychology
front,
the a dramatic sounding terminology of
replication crisis, but really just

[1:01:19]
replication crisis, but really just
the at times
this this effect that at times studies
do not are not fully generalizable. They
don't
if you're being polite. It's worse than

[1:01:32]
if you're being polite. It's worse than
that. But
is it? So I'm actually not fully
familiar
to the degree how bad it is, right? So
what do you think is the source? Where
do you think
I think I know what's going on actually.
I mean I have a theory about what's

[1:01:46]
I mean I have a theory about what's
going on and what's going on
is that there is first of all a very
important distinction between two types
of experiments and one type is within
subject. So it's the same person has two

[1:02:03]
subject. So it's the same person has two
experimental conditions and the other
type is between subjects where some
people have this condition other people
have that condition. They're different
worlds and between subject experiments

[1:02:16]
worlds and between subject experiments
are much harder to predict and much
harder to anticipate. And the reason
uh and they're also more expensive
because you need more people and it's
it's just so between subject experiments

[1:02:33]
it's just so between subject experiments
is where the problem is.
Uh it's not so much and within subject
experiments, it's really between and
there is a very good reason why the
intuitions of researchers about between

[1:02:47]
intuitions of researchers about between
subject experiments are wrong. And
that's because when you are a
researcher, you're in a within subject
situation. That is you are imagining the
two conditions and you see the causality

[1:03:02]
two conditions and you see the causality
and you feel it and but in the between
subjects condition they don't they see
they live in one condition and the other
one is just nowhere. So our intuitions

[1:03:16]
one is just nowhere. So our intuitions
are very weak about between subject
experiments and that I think is
something that people haven't realized
and and in addition
because of that we have no idea about

[1:03:31]
because of that we have no idea about
the power of uh manipulations of
experimental manipulations because the
same manipulation is much more powerful
when when you are in the two conditions
than when you live in only one

[1:03:45]
than when you live in only one
condition. And so the experimenters have
very poor intuitions about between
subject experiments. And and there is
something else which is very important I
think which is that almost all

[1:04:01]
think which is that almost all
psychological hypotheses are true. That
is in the sense that you know
directionally
if you have a hypothesis that A really
causes B that that it's not true that A

[1:04:16]
causes B that that it's not true that A
causes the opposite of B maybe A just
has very little effect but hypotheses
are true
mostly except mostly they're very weak
they're much weaker than you think when

[1:04:30]
they're much weaker than you think when
you are having images of so the reason
I'm excited about that is that I
recently heard about
uh some some friends of mine who

[1:04:46]
uh some some friends of mine who
uh they essentially funded 53 studies of
behavioral change
by 20 different teams of people with a
very precise objective of changing the
number of time that people go to the

[1:05:00]
number of time that people go to the
gym. But you know and
and the success rate was zero.
Not one of the 53 studies worked. Now
what's interesting about that is those

[1:05:16]
what's interesting about that is those
are the best people in the field and
they have no idea what's going on. So
they're not calibrated. They think that
it's going to be powerful because they
can imagine it. But actually it's just
weak because the you are focusing on on

[1:05:33]
weak because the you are focusing on on
your manipulation and it feels powerful
to you.
There's a thing that I've written about
that's called the focusing illusion.
That is that when you think about
something it looks very important

[1:05:46]
something it looks very important
more important than it really is. But if
you don't see that effect the 53 studies
doesn't that mean you just report that?
So what what's I guess the solution to
that? Well, I mean the

[1:06:01]
that? Well, I mean the
the solution is for people to trust
their intuitions less or to try out
their intuitions before I mean
experiments have to be pre-registered
and by the time you run an experiment

[1:06:16]
and by the time you run an experiment
you have to be committed to it and you
have to run the experiment seriously
enough and uh in a public and so this is
happening. The interesting thing is uh

[1:06:30]
happening. The interesting thing is uh
what what happens before and how do
people prepare themselves and how they
run pilot experiments. It's going to
train the way psychology is done and
it's already happening.
Do you have a hope for this might
connect to uh the this study sample

[1:06:48]
connect to uh the this study sample
size?
Yeah.
Uh do you have a hope for the internet
or I mean you know this is really
happening murk. Yeah.
Uh, everybody is running experiments on
MTOK and and it's very cheap and very

[1:07:02]
MTOK and and it's very cheap and very
effective. So,
do you think that changes psychology
essentially because you're think you can
now run 10,000 subjects
eventually it will. Yeah,
I mean I you know I can't put my finger
on how exactly but it's that's been true

[1:07:19]
on how exactly but it's that's been true
in psychology with whenever an important
new method came in it changes the feel
so and and murk is really a method
because it it makes it very much easier

[1:07:32]
because it it makes it very much easier
to do some to do some things. Is there
uh undergrad students will ask me you
know how big a neural network should be
for a particular problem. So let me ask
you an equivalent equivalent question.

[1:07:46]
you an equivalent equivalent question.
Uh how big how many subjects a study
have for it to have a conclusive result?
Well it depends on the strength of the
effect. So if you're studying visual
perception

[1:08:00]
perception
or the perception of color, many of the
the classic results in in visual in
color perception were done on three or
four people and I think one of them was
color blind but or partly color blind
but on vision you know you it's highly

[1:08:19]
but on vision you know you it's highly
related many
uh people don't need a lot of
replications for some type of of
neurological
experiment. Neuro

[1:08:31]
experiment. Neuro
uh when you're studying weaker phenomena
and especially when you're studying them
between subjects then you need a lot
more subjects than people have been
running and that is that's one of the

[1:08:46]
running and that is that's one of the
thing that are happening in psychology
now is that the power the statistical
power of the experiments is is
increasing rapidly. Does the between
subject as the number of subjects goes
to infinity approach?
Well, I mean you know goes to infinity

[1:09:02]
Well, I mean you know goes to infinity
is exaggerated but people the standard
number of subjects for an experiment
psychology with 30 or 40 and for a weak
effect that's simply not enough

[1:09:17]
effect that's simply not enough
and you may need a couple of hundred. I
mean it's that that sort of uh order of
magnitude.
What are the major disagreements

[1:09:31]
What are the major disagreements
and theories and effects that you've
observed throughout your career that
still stand today? You've worked on
several fields.
Yeah.
What still is out there as as major
disagreement that pops into your mind?

[1:09:46]
disagreement that pops into your mind?
And I've had one extreme experience of,
you know, controversy with somebody who
really doesn't like the work that Amoski
and I did and and he's been after us for
30 years or more at least.

[1:10:02]
30 years or more at least.
Do you want to talk about it?
Well, I mean, his name is Good Giganzer.
He's a well-known German psychologist.
And that's the one controversy I have
which I

[1:10:15]
which I
it's been unpleasant and and no I don't
particularly want to talk about it
but is there is there open questions
even in your own mind every once in a
while you know uh we talked about
semi-autonomous vehicles in my own mind

[1:10:30]
semi-autonomous vehicles in my own mind
I see what the data says but I also
constantly torn do you have things where
you your studies have found something
but you're also intellectual torn about
what it means and there's maybe been
maybe disagreements without your within

[1:10:45]
maybe disagreements without your within
your own mind about particular things.
I mean it's you know one of the things
that are interesting is how difficult it
is for people to change their mind
essentially
uh you know once they are committed

[1:11:00]
uh you know once they are committed
people just don't change their mind
about anything that matters and that is
surprisingly but it's true about
scientists. So the controversy that I
described uh you know that's been going
on like 30 years and it's never going to
be resolved

[1:11:16]
be resolved
uh and you build a system and you live
within that system and other other
systems of ideas look foreign to you and
and and there is very little contact and
very little mutual influence that

[1:11:31]
very little mutual influence that
happens a fair amount. Do you have a
hopeful
advice or message on that? We thinking
about science, thinking about politics,
thinking about things that have impact

[1:11:45]
thinking about things that have impact
on this world. How can we change our
mind?
I think that I mean on things that
matter you know which are political or
rel political or religious and people
just don't don't change their mind

[1:12:02]
just don't don't change their mind
and by and large and there's very little
that you can do about it. Uh the what
does happen is that if leaders change
their minds. So for example,

[1:12:16]
their minds. So for example,
the public, the American public doesn't
really believe in climate change,
doesn't take it very seriously, but if
some religious leaders decided this is a
major threat to humanity, that would

[1:12:30]
major threat to humanity, that would
have a big effect. So that we we have
the opinions that we have not because we
know why we have them, but because we
trust some people and we don't trust
other people. And uh so it's much less
about evidence than it is about stories.

[1:12:49]
about evidence than it is about stories.
So the way one way to change your mind
isn't at the individual level is that
the leaders of the communities you look
up with the stories change and therefore
your mind changes with them.

[1:13:01]
your mind changes with them.
So there's a guy named Alan Touring came
up with a touring test.
Yeah.
Uh what what do you think is a good test
of intelligence? Perhaps we're drifting
in a topic that we're um maybe

[1:13:17]
in a topic that we're um maybe
philosophizing about, but what do you
think is a good test for intelligence
for an artificial intelligence system?
Well, the standard definition of you
know of artificial general intelligence

[1:13:30]
know of artificial general intelligence
that it can do anything that people can
do and it can do them better.
Yes. What what we are seeing is that in
many domains you have domain specific
uh and

[1:13:45]
uh and
you know devices or programs or software
and they beat people easily in specified
way. What we are very far from is that
general ability a general purpose
intelligence. So we we

[1:14:02]
intelligence. So we we
in machine learning people are
approaching something more general. I
mean for Alpha Zero was was much more
general than than Alpha Go. And

[1:14:15]
general than than Alpha Go. And
but it's still extraordinarily narrow
and specific in what it can do. So,
so we're quite far from from something
that can in every domain think like a
human except better.

[1:14:30]
human except better.
What aspect? So, the the touring test
has been criticized. It's natural
language conversation.
Yeah.
That is too simplistic. Uh it's easy to
quote unquote pass under under
constraints specified. What aspect of
conversation would impress you if you

[1:14:46]
conversation would impress you if you
heard it? Is it humor? Is it
what what would impress the heck out of
you if uh if you saw it in conversation?
Yeah, I mean certainly wit would you
know wit would be impressive

[1:15:00]
know wit would be impressive
uh and and and
humor would be more impressive than just
factual conversation which I think is is
easy and illusions would be interesting
and

[1:15:16]
and
metaphors would be interesting. I mean
but new metaphors not practiced
metaphors. So there is a lot that you
know would be sort of impressive if that

[1:15:30]
know would be sort of impressive if that
is completely natural in conversation
but that you really wouldn't expect.
Does the possibility of creating an a
human level intelligence or superhuman
level intelligence system excite you?
Scare you?
Well, I mean how does it make you feel?

[1:15:45]
Well, I mean how does it make you feel?
Uh I find the whole thing fascinating.
Absolutely fascinating. exciting
I think and exciting. It's also
terrifying, you know, but but I'm not
going to be around to see it. And uh so

[1:16:03]
going to be around to see it. And uh so
I'm curious about what is happening now,
but I also know that that predictions
about it are silly. Uh we really have no
idea what it will look like 30 years

[1:16:15]
idea what it will look like 30 years
from now. No idea.
Speaking of silly, bordering on the
profound, they may ask the question of
in your view, what is the meaning of it
all? The meaning of life? He's a

[1:16:31]
all? The meaning of life? He's a
descendant of great apes that we are.
Why? What drives us as a civilization,
as a human being, as a force behind
everything that you've observed and
studied?
Is there any answer or is it all just a

[1:16:46]
Is there any answer or is it all just a
beautiful mess?
There is no answer that that I can
understand.
Uh and I'm not and I'm not actively
looking for one. Um

[1:17:00]
looking for one. Um
do you think an answer exists?
No, there is no answer that we can
understand. I'm not qualified to speak
about what we cannot understand. But
there is I know that we cannot
understand reality you know and

[1:17:16]
understand reality you know and
I mean there are a lot of thing that we
can do I mean you know gravity waves I
mean that's that's a big moment for
humanity and
when you imagine that ape you know being
able to to go back to the big bang

[1:17:31]
able to to go back to the big bang
that's that's
but but the why
yeah the why
bigger than us
the boy is hopeless. Really,
Danny, thank you so much. It was an
honor. Thank you for speaking today.
Thank you.

[1:17:45]
Thank you.
Thanks for listening to this
conversation and thank you to our
presenting sponsor, Cash App. Download
it. Use code Lex Podcast. You'll get $10
and $10 will go to First, a STEM
education nonprofit that inspires
hundreds of thousands of young minds to

[1:18:00]
hundreds of thousands of young minds to
become future leaders and innovators. If
you enjoy this podcast, subscribe on
YouTube. Give it five stars on Apple
Podcast, follow on Spotify, support it
on Patreon, or simply connect with me on
Twitter.
And now, let me leave you with some

[1:18:15]
And now, let me leave you with some
words of wisdom from Daniel Conorman.
Intelligence is not only the ability to
reason, it is also the ability to find
relevant material in memory and to
deploy attention when needed.
Thank you for listening and hope to see

[1:18:30]
Thank you for listening and hope to see
you next time.